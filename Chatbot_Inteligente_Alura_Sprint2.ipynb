{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9A2oSLDivrbN",
        "qQEIdACV2IE4",
        "kDRIGcy62JuE",
        "_RnqKO9ChOpY",
        "IVJ6UmQV3n4T"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JulioLaz/chatbot_spring_02/blob/main/Chatbot_Inteligente_Alura_Sprint2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Configurar ambiente"
      ],
      "metadata": {
        "id": "9A2oSLDivrbN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UD0OObFGtZVJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc412322-a4c5-4d24-9d01-b8c19f4d7a31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-25 22:26:16.348744: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting es-core-news-md==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.6.0/es_core_news_md-3.6.0-py3-none-any.whl (42.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-md==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (2.1.3)\n",
            "Installing collected packages: es-core-news-md\n",
            "Successfully installed es-core-news-md-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_md')\n",
            "Collecting jellyfish\n",
            "  Downloading jellyfish-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jellyfish\n",
            "Successfully installed jellyfish-1.0.1\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m123.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.2 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.3)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184487 sha256=3bc95ec36b16d68ccd6db964629df8e61b37ecbe5bc73eba63bab03a40c9bd10\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Instalando bibliotecas\n",
        "import pandas as pd\n",
        "import re, os, random, pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import spacy\n",
        "!python -m spacy download es_core_news_md\n",
        "!pip install jellyfish\n",
        "import jellyfish\n",
        "!pip install transformers\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "!pip install python-docx\n",
        "from docx import Document\n",
        "import csv\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "#Definiendo variables del proyecto:\n",
        "nlp = spacy.load('es_core_news_md')\n",
        "\n",
        "#Conectando al Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder = '/content/drive/MyDrive/Chatbot'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Importar verbos"
      ],
      "metadata": {
        "id": "qQEIdACV2IE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "archivo_pickle_verbos = \"/content/drive/MyDrive/Chatbot/verbos/lista_verbos.pickle\"\n",
        "archivo_pickle_verbos_irregulares = \"/content/drive/MyDrive/Chatbot/verbos/verbos_irregulares.pickle\"\n",
        "\n",
        "# Importar la lista_verbos:\n",
        "with open(archivo_pickle_verbos, \"rb\") as archivo:\n",
        "        lista_verbos = pickle.load(archivo)\n",
        "\n",
        "# Importar el diccionario:\n",
        "with open(archivo_pickle_verbos_irregulares, \"rb\") as archivo_irr:\n",
        "        lista_verbos_irregulares = pickle.load(archivo_irr)\n",
        "\n",
        "\n",
        "print(lista_verbos)\n",
        "print('---------------------------------')\n",
        "print(lista_verbos_irregulares)"
      ],
      "metadata": {
        "id": "8DDVG_F22IvU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0682b7c2-0705-440b-bf75-73af74053f7c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['parar', 'recomendar', 'cancelar', 'fanatizar', 'amaran o amasen', 'exponer', 'obedecer', 'quejar', 'echar', 'legitimar', 'perjudicar', 'organizar', 'molar', 'objetar', 'considerar', 'golear', 'mover', 'acertar', 'reunir', 'regir', 'ilusionar', 'simpatizar', 'conjeturar', 'helar', 'quitar', 'amariamos', 'destacar', 'llegar', 'sincronizar', 'lesionar', 'seducir', 'asistir', 'conservar', 'acordar', 'salvar', 'relucir', 'graduar', 'forzar', 'dar', 'deplorar', 'batear', 'mofar', 'estropear', 'aplastar', 'wasapeo', 'gestionar', 'suprimir', 'gruñir', 'progresar', 'suscribir', 'noticiar', 'cavar', 'alejar', 'galopar', 'virar', 'medir', 'actualizar', 'humanizar', 'convivir', 'gratificar', 'digerir', 'tocar', 'zonificar', 'amariais', 'aterrizar', 'hojear', 'cometer', 'sufrir', 'reciclar', 'obturar', 'divertir', 'ondear', 'listar', 'determinar', 'alentar', 'sumar', 'reflexionar', 'ames', 'anotar', 'mitificar', 'escribir', 'ilustrar', 'obtener', 'subir', 'socorrer', 'desprender', 'agregar', 'gandulear', 'improvisar', 'traquetear', 'idealizar', 'pescar', 'despertarse', 'decorar', 'ceñir', 'asar', 'balbucear', 'noquear', 'amaremos', 'desconectar', 'tambalear', 'vengar', 'bolear', 'trabar', 'resplandecer', 'vagar', 'pasmar', 'susurrar', 'amamos', 'suceder', 'pelar', 'toser', 'preocupar', 'martillar', 'zarpar', 'repartir', 'costar', 'llenar', 'contradecir', 'trasplantar', 'embrujar', 'exonerar', 'wasapearia', 'fabular', 'alquilar', 'capturar', 'granular', 'instruir', 'roer', 'ufanarse', 'filtrar', 'pulsar', 'proporcionar', 'fijar', 'fallar', 'identificar', 'mermar', 'agradar', 'arder', 'desmerecer', 'arquear', 'desquitar', 'contar', 'pesar', 'rendir', 'exfoliar', 'gritar', 'hartar', 'pegar', 'irrumpir', 'resaltar', 'lagrimear', 'filosofar', 'perdonar', 'defender', 'zaherir', 'corromper', 'girar', 'titular', 'guadañar', 'evadir', 'traspasar', 'homenajear', 'dominar', 'variar', 'disminuir', 'wasapea', 'querellar', 'tiritar', 'excavar', 'opinar', 'comentar', 'prohibir', 'repasar', 'vislumbrar', 'alegrar', 'fotografiar', 'mosquear', 'hechizar', 'nadar', 'hastiar', 'recluyen', 'galvanizar', 'planificar', 'fisurar', 'fluctuar', 'comenzar', 'recrudecer', 'exagerar', 'colonizar', 'calcular', 'exterminar', 'desayunar', 'lamer', 'fluir', 'invadir', 'encantar', 'combinar', 'apagar', 'televisar', 'probar', 'aconsejar', 'necesitar', 'disparar', 'manejar', 'oxidar', 'devenir', 'ostentar', 'bucear', 'bautizar', 'moler', 'reir', 'crujir', 'shockear', 'nosotros', 'empezar', 'compensar', 'estatizar', 'alumbrar', 'formalizar', 'imaginar', 'construir', 'disuadir', 'iluminar', 'condimentar', 'estrenar', 'originar', 'abastecer', 'prolongar', 'usurar', 'inhalar', 'declinar', 'ensuciar', 'humillar', 'examinar', 'exteriorizar', 'amare', 'gobernar', 'tramar', 'oscilar', 'huir', 'esperar', 'infringir', 'predecir', 'wasapearian', 'alimentar', 'creer', 'niñear', 'zaparrastrar', 'quebrar', 'wasapeareis', 'almorzar', 'expresar', 'asentir', 'resolver', 'bailar', 'chocar', 'deprimir', 'obsequiar', 'sobrar', 'fluye', 'arañar', 'redimir', 'justiciar', 'acusar', 'robar', 'amaramos o amasemos', 'quemar', 'urdir', 'enamorarse', 'ocultar', 'salir', 'fantasear', 'ovacionar', 'apoyar', 'construyo', 'pedalear', 'galantear', 'lacrar', 'wasapearais o wasapeaseis', 'wasapeen', 'disentir', 'envejecer', 'radiar', 'asfixiar', 'bromear', 'amar', 'masacrar', 'representar', 'proclamar', 'alterar', 'incluyeran', 'inscribir', 'aligerar', 'querer', 'vapulear', 'traer', 'heredar', 'tararear', 'oscurecer', 'disminuyen', 'nutrir', 'oficializar', 'guiar', 'traducir', 'escoger', 'descarrilar', 'colorear', 'luchar', 'wasapeamos', 'ama', 'resarcir', 'convidar', 'liar', 'narrar', 'desconocer', 'ameis', 'protagonizar', 'refrescar', 'doblar', 'montar', 'ensayar', 'comer', 'obrar', 'fruncir', 'marinar', 'subrayar', 'replicar', 'teclear', 'auxiliar', 'litigar', 'sobrevivir', 'obstruir', 'vibrar', 'hablar', 'embestir', 'merendar', 'refutar', 'dirigir', 'hacinar', 'aspirar', 'propagar', 'sobreseer', 'educar', 'centrar', 'extender', 'redistribuya', 'plantar', 'aprobar', 'practicar', 'alarmar', 'bordar', 'instrumentar', 'universalizar', 'desteñir', 'glosar', 'magnificar', 'urgir', 'procurar', 'rebatir', 'quedar', 'razonar', 'macanear', 'renacer', 'repeler', 'fundir', 'rugir', 'subyacer', 'datar', 'incidir', 'frenar', 'negociar', 'definir', 'estallar', 'preexistir', 'turnar', 'caber', 'tranquilizar', 'disolver', 'localizar', 'varar', 'refaccionar', 'chupar', 'programar', 'zigzaguear', 'lookear', 'concluir', 'grisear', 'dividir', 'violentar', 'brincar', 'alborotar', 'dictar', 'casar', 'linchar', 'tricotar', 'temer', 'novelar', 'kickear', 'urbanizar', 'partir', 'mensurar', 'diferenciar', 'emancipar', 'wasapeasteis', 'aburrir', 'sacudir', 'confundir', 'añorar', 'constituyeran', 'ignorar', 'judicializar', 'solicitar', 'nasalizar', 'escuchar', 'desistir', 'esquiar', 'orar', 'cayo', 'bracear', 'herrumbrar', 'wasapean', 'destituyeron', 'trastornar', 'blindar', 'adquirir', 'distribuir', 'balancear', 'wasapeariais', 'instituyo', 'disimular', 'oxigenar', 'testimoniar', 'entusiasmar', 'desmentir', 'fragmentar', 'exhortar', 'felicitar', 'condicionar', 'vacunar', 'encontrar', 'mudar', 'mostrar', 'afilar', 'armar', 'concluyeron', 'mancillar', 'explotar', 'bajar', 'lacerar', 'chatear', 'diseñar', 'valuar', 'mecer', 'adorar', 'exigir', 'birlar', 'suplir', 'prevenir', 'registrar', 'terminar', 'jugar', 'aguardar', 'obviar', 'ellos', 'salar', 'higienizar', 'gatear', 'jerarquizar', 'presentar', 'yerran', 'hibernar', 'venir', 'clavar', 'olvidar', 'gimotear', 'asquear', 'roncar', 'wasapeariamos', 'resultar', 'vaciar', 'languidecer', 'intuyo', 'laquear', 'comprometer', 'enganchar', 'enquistar', 'entrar', 'nominar', 'impregnar', 'mutilar', 'fusilar', 'paralizar', 'tramitar', 'latir', 'vociferar', 'andar', 'arrojar', 'espirar', 'redactar', 'hornear', 'adornar', 'wasapeabas', 'ordenar', 'habilitar', 'prender', 'comerciar', 'transcribir', 'saciar', 'comunicar', 'maldecir', 'conectar', 'juntar', 'anexar', 'impresionar', 'conducir', 'cerrar', 'murmurar', 'firmar', 'merecer', 'neutralizar', 'consolidar', 'obnubilar', 'germinar', 'activar', 'comprar', 'wasapearias', 'reservar', 'sudar', 'atribuyo', 'nacionalizar', 'exhibir', 'yacer', 'contribuyan', 'impedir', 'familiarizar', 'intuir', 'exorcizar', 'obstaculizar', 'flotar', 'amaria', 'pasear', 'humedecer', 'barajar', 'dudar', 'celebrar', 'ayudar', 'adelantar', 'sepultar', 'jubilar', 'depositar', 'preservar', 'disfrazar', 'admitir', 'abatir', 'escalar', 'numerar', 'golpear', 'eliminar', 'llorisquear', 'estar', 'unir', 'sobresalir', 'exceptuar', 'amasteis', 'completar', 'expirar', 'glasear', 'lijar', 'obstruyen', 'expulsar', 'ofender', 'finalizar', 'restaurar', 'pisar', 'patinar', 'cepillar', 'lapidar', 'balar', 'crackear', 'huyeron', 'vaporizar', 'fastidiar', 'suponer', 'diluye', 'ocasionar', 'soler', 'implementar', 'parir', 'hemos wasapeado', 'llamar', 'estimar', 'reportar', 'satirizar', 'aproximar', 'leer', 'diluir', 'brindar', 'engordar', 'restablecer', 'retrasar', 'haya', 'enfriar', 'acariciar', 'vedar', 'lexicalizar', 'vos', 'equivocar', 'declarar', 'arrastrar', 'leñar', 'desafinar', 'garabatear', 'cambiar', 'yendo', 'deber', 'ofrecer', 'afinar', 'mendigar', 'confesar', 'gasificar', 'torcer', 'enchufar', 'incluir', 'retribuir', 'has wasapeado', 'sabotear', 'emitir', 'pillar', 'husmear', 'aportar', 'presumir', 'noblecer', 'privatizar', 'gravitar', 'rehusar', 'sobreexcitar', 'conseguir', 'recluir', 'explicitar', 'levar', 'tasar', 'reprochar', 'referir', 'usurpar', 'coincidir', 'rascar', 'maquillarse', 'distinguir', 'acabar', 'recordar', 'portar', 'presionar', 'aquejar', 'excitar', 'detener', 'amarian', 'garantizar', 'imprimir', 'atender', 'indisponer', 'destruir', 'aliñar', 'reclamar', 'volar', 'codificar', 'enamorar', 'desnudar', 'encender', 'menear', 'ventilar', 'puntualizar', 'pulir', 'sorprender', 'existir', 'tronar', 'maniatar', 'pinchar', 'abolir', 'ocluir', 'climatizar', 'fundar', 'inspirar', 'sonorizar', 'legar', 'migrar', 'notar', 'excretar', 'adjuntar', 'tatuar', 'elevar', 'fingir', 'solucionar', 'pronunciar', 'procesar', 'ulcerar', 'militarizar', 'amaran', 'dibujar', 'vestir', 'reconstruyeron', 'personalizar', 'revertir', 'retirar', 'concentrar', 'amenizar', 'editar', 'hilar', 'administrar', 'ejecutar', 'financiar', 'divorciar', 'borrar', 'vincular', 'sumergir', 'respetar', 'generar', 'afluyen', 'nortear', 'hermanar', 'competir', 'curtir', 'diferir', 'taladrar', 'favorecer', 'preguntar', 'calmar', 'malgastar', 'niquelar', 'pensar', 'malcriar', 'vayamos', 'sucumbir', 'detestar', 'retribuyan', 'complicar', 'reducir', 'aprovechar', 'integrar', 'zamarrear', 'rockear', 'depender', 'tintar', 'advertir', 'rozar', 'acotar', 'recuperar', 'ganar', 'culpar', 'liberar', 'recopilar', 'preferir', 'lucir', 'vacilar', 'ha wasapeado', 'optimizar', 'remplazar', 'saltar', 'lamentar', 'enojar', 'accionar', 'confiar', 'gotear', 'tallar', 'apostar', 'regalar', 'delinquir', 'ungir', 'enseñar', 'decidir', 'reaccionar', 'multiplicar', 'romper', 'untar', 'trotar', 'reflejar', 'enumerar', 'circular', 'wasapee', 'exiliar', 'honrar', 'suspirar', 'guillotinar', 'leyera', 'zorrear', 'encoger', 'avisar', 'calentar', 'asegurar', 'barnizar', 'hincar', 'abandonar', 'enhebrar', 'baldear', 'vivir', 'marear', 'fotocopiar', 'burbujear', 'relacionar', 'remitir', 'orquestar', 'pintar', 'plantear', 'fortalecer', 'concebir', 'situar', 'dimitir', 'matar', 'radicalizar', 'inferir', 'sostener', 'soportar', 'nacer', 'destruyo', 'perfeccionar', 'nominalizar', 'besar', 'destinar', 'exhalar', 'glorificar', 'ahorrar', 'criminalizar', 'empujar', 'conocer', 'conquistar', 'atar', 'movilizar', 'restar', 'coronar', 'sanar', 'acomodar', 'inhabilitar', 'trasladar', 'atribuir', 'estimular', 'tratar', 'fabricar', 'hipotecar', 'degustar', 'bifurcar', 'señalar', 'pretender', 'acostar', 'apreciar', 'contactar', 'lubricar', 'inaugurar', 'planear', 'grabar', 'fregar', 'llevar', 'aplicar', 'equiparar', 'gemir', 'stalkear', 'ultimar', 'acceder', 'descender', 'reproducir', 'mantener', 'funcionar', 'rodar', 'cantar', 'proyectar', 'coordinar', 'rejuvenecer', 'fomentar', 'entretener', 'absorber', 'interferir', 'conceder', 'volver', 'inspeccionar', 'molestar', 'pender', 'oler', 'escupir', 'influye', 'magullar', 'seguir', 'trabajar', 'abrazar', 'invitar', 'revelar', 'inclinar', 'osar', 'orientar', 'bostezar', 'realizar', 'ablandar', 'omitir', 'penetrar', 'sospechar', 'sobrescribir', 'negar', 'limar', 'entrenar', 'volcar', 'pelear', 'elegir', 'persuadir', 'ingresar', 'explorar', 'quintuplicar', 'admirar', 'granizar', 'cuidar', 'invocar', 'castellanizar', 'causar', 'estilizar', 'ingerir', 'equilibrar', 'discriminar', 'informar', 'beneficiar', 'augurar', 'eyectar', 'he wasapeado', 'liderar', 'alegrarse', 'frustrar', 'instruye', 'inventar', 'picar', 'enterar', 'pronosticar', 'recibir', 'zampar', 'exclamar', 'aprender', 'ella', 'laborar', 'recelar', 'menospreciar', 'reconquistar', 'vilipendiar', 'malhumorar', 'hallar', 'maquillar', 'saber', 'gravar', 'aflojar', 'tomar', 'excluir', 'operar', 'descubrir', 'extenuar', 'tensar', 'purificar', 'anular', 'ejercitar', 'tragar', 'concurrir', 'ultrajar', 'asociar', 'relatar', 'sumir', 'consultar', 'expender', 'madurar', 'despedir', 'nivelar', 'visitar', 'guisar', 'usufructuar', 'zanjar', 'vallar', 'vejar', 'enriquecer', 'zafar', 'estremecer', 'recorrer', 'broncear', 'obsesionar', 'oprimir', 'escapar', 'mejorar', 'otear', 'impartir', 'halagar', 'buscar', 'cultivar', 'saquear', 'donar', 'perfumar', 'eximir', 'remover', 'wasapeemos', 'acelerar', 'enfurecer', 'tardar', 'prometer', 'clarear', 'ocurrir', 'bombear', 'respirar', 'jactar', 'torturar', 'veranear', 'transportar', 'presentir', 'elaborar', 'indexar', 'ironizar', 'vetar', 'boicotear', 'meditar', 'estirar', 'negrear', 'descongelar', 'atacar', 'verter', 'soltar', 'wasapeaba', 'jadear', 'fertilizar', 'inmiscuyan', 'tolerar', 'iniciar', 'enfatizar', 'gesticular', 'vagabundear', 'regatear', 'devorar', 'cuchichear', 'zancadillear', 'sindicalizar', 'acreditar', 'tumbar', 'encuestar', 'excluyen', 'botar', 'instalar', 'apuntar', 'subdividir', 'alojar', 'tener', 'otorgar', 'rehuyo', 'apurar', 'freir', 'pregonar', 'vencer', 'complejizar', 'nevar', 'entrometer', 'herrar', 'malograr', 'wasapearan', 'suspender', 'acampar', 'hundir', 'prestar', 'ubicar', 'deletrear', 'responder', 'poseer', 'deponer', 'resfriarse', 'zalear', 'maravillar', 'optar', 'interponer', 'endurecer', 'fortificar', 'desatar', 'irradiar', 'wasapeare', 'reinar', 'excomulgar', 'vendar', 'cifrar', 'eludir', 'stockear', 'consumir', 'granjear', 'titilar', 'flaquear', 'requerir', 'yuxtaponer', 'tentar', 'aliviar', 'extraer', 'explayar', 'inundar', 'retorcer', 'sujetar', 'pulverizar', 'contextualizar', 'habitar', 'utilizar', 'rebobinar', 'holgazanear', 'wasapeeis', 'llover', 'abrochar', 'abusar', 'seleccionar', 'aguantar', 'wasapeara o wasapease', 'observar', 'ovular', 'gerenciar', 'repetir', 'amenazar', 'deteriorar', 'cesar', 'condenar', 'trazar', 'bramar', 'formar', 'almacenar', 'guerrear', 'destituir', 'lastimar', 'errar', 'violar', 'besuquear', 'mezclar', 'modelar', 'obligar', 'amen', 'importar', 'holgar', 'wasapeais', 'izar', 'jabonar', 'exasperar', 'disputar', 'dormir', 'agarrar', 'debatir', 'masajear', 'acompañar', 'wasapeas', 'chasquear', 'zambullir', 'encajar', 'denigrar', 'jalonar', 'juerguear', 'restituir', 'clasificar', 'afirmar', 'amaras o amases', 'remar', 'tirar', 'ligar', 'incumplir', 'moldear', 'decir', 'ajustar', 'madrugar', 'simbolizar', 'batir', 'traficar', 'empequeñecer', 'asesorar', 'wasapearan o wasapeasen', 'tender', 'escabullir', 'entrevistar', 'aquietar', 'aturdir', 'criticar', 'notificar', 'formular', 'marginar', 'quebrantar', 'intimidar', 'engrasar', 'proceder', 'zapar', 'flexibilizar', 'apartar', 'corresponder', 'elogiar', 'juguetear', 'lloviznar', 'judicar', 'atemorizar', 'reprender', 'ver', 'publicar', 'agotar', 'vosotros', 'mentir', 'inhibir', 'explicar', 'guiñar', 'abortar', 'transpirar', 'intervenir', 'sacrificar', 'marchar', 'ir', 'ladrar', 'traicionar', 'reconstituyo', 'animar', 'asomar', 'librar', 'zurcir', 'esquivar', 'lavar', 'renunciar', 'disculpar', 'grajear', 'emplear', 'simular', 'rellenar', 'enfermar', 'igualar', 'combatir', 'embellecer', 'injuriar', 'anunciar', 'amareis', 'desear', 'perder', 'someter', 'apelar', 'investigar', 'cuajar', 'desplazar', 'establecer', 'confluyan', 'embaucar', 'señalizar', 'orinar', 'arrestar', 'fallecer', 'hacer', 'interrumpir', 'voltear', 'complacer', 'sintonizar', 'amarais o amaseis', 'bañar', 'sonar', 'horadar', 'grillar', 'tejer', 'ustedes', 'imponer', 'caminar', 'macerar', 'expandir', 'zapatear', 'rechazar', 'herir', 'asustar', 'posar', 'encargar', 'aclarar', 'platicar', 'sextuplicar', 'venerar', 'relajar', 'encerrar', 'repercutir', 'viajar', 'embrollar', 'amais', 'entristecer', 'morir', 'callar', 'vitorear', 'galardonar', 'subsistir', 'devolver', 'divisar', 'acoger', 'hurgar', 'manipular', 'mediar', 'entorpecer', 'exceder', 'anhelar', 'trepar', 'humear', 'adaptar', 'cocinar', 'amara o amase', 'gambetear', 'uniformar', 'estornudar', 'abrir', 'introducir', 'atrever', 'licuar', 'festejar', 'odiar', 'agitar', 'percibir', 'analizar', 'satisfacer', 'hidratar', 'copiar', 'unisonar', 'acortar', 'triturar', 'interpretar', 'legalizar', 'valorar', 'dejar', 'gustar', 'hurtar', 'usar', 'intentar', 'brotar', 'bambolear', 'machucar', 'faltar', 'ellas', 'narcotizar', 'justificar', 'lograr', 'coleccionar', 'burlar', 'lindar', 'brillar', 'aislar', 'indicar', 'aplaudir', 'cosechar', 'sacar', 'falsificar', 'hackear', 'telefonear', 'asumir', 'marchitar', 'malherir', 'banalizar', 'extinguir', 'desaconsejar', 'comprender', 'proveer', 'manifestar', 'parpadear', 'trackear', 'derretir', 'deshacer', 'arreglar', 'peinar', 'emigrar', 'jactarse', 'carecer', 'saludar', 'balear', 'fracasar', 'reparar', 'saborear', 'colaborar', 'cansar', 'durar', 'menguar', 'protestar', 'influir', 'noviar', 'resistir', 'enredar', 'ensanchar', 'separar', 'tornear', 'afeitar', 'trajinar', 'boxear', 'habeis', 'adivinar', 'revolver', 'naufragar', 'batallar', 'oyeramos', 'bufar', 'flirtear', 'hospitalizar', 'ulular', 'demostrar', 'sugerir', 'mencionar', 'patear', 'normalizar', 'acosar', 'surtir', 'jaquear', 'profetizar', 'recurrir', 'nombrar', 'martirizar', 'esterilizar', 'cenar', 'fusionar', 'infundir', 'hostigar', 'prostituya', 'aumentar', 'amarias', 'fiar', 'fascinar', 'incrementar', 'desarrollar', 'extrañar', 'rodear', 'distraer', 'hipnotizar', 'cooperar', 'constatar', 'regresar', 'convencer', 'ceder', 'hilvanar', 'temperar', 'compartir', 'añadir', 'confirmar', 'reponer', 'envolver', 'sintetizar', 'atrasar', 'olfatear', 'desvestir', 'vigilar', 'hamacar', 'malentender', 'contestar', 'esclarecer', 'llamear', 'compilar', 'mojar', 'facilitar', 'estacionar', 'resumir', 'pedir', 'atenuar', 'renovar', 'oponer', 'envidiar', 'inmigrar', 'reconocer', 'expiar', 'bastar', 'descomponer', 'sustituir', 'ocupar', 'licenciar', 'afectar', 'recitar', 'zaracear', 'maniobrar', 'readmitir', 'inmiscuir', 'nacarar', 'zunchar', 'motivar', 'invertir', 'asesinar', 'difundir', 'participar', 'juzgar', 'cautivar', 'aceptar', 'convertir', 'preparar', 'horrorizar', 'nublar', 'equivaler', 'llorar', 'visualizar', 'impulsar', 'exportar', 'importunar', 'zumbar', 'inquietar', 'esclavizar', 'cubrir', 'retener', 'entrañar', 'irrigar', 'privar', 'excusar', 'objetivar', 'evitar', 'cazar', 'proponer', 'enloquecer', 'hinchar', 'incorporar', 'beber', 'naturalizar', 'acostumbrar', 'lanzar', 'incurrir', 'luxar', 'exaltar', 'rasquetear', 'fumar', 'conversar', 'interesar', 'curar', 'wasapearemos', 'valer', 'ame', 'bonificar', 'wasapearon', 'levantar', 'limpiar', 'vender', 'restringir', 'revisar', 'amas', 'candar', 'decepcionar', 'manotear', 'wasapees', 'medicar', 'florecer', 'liquidar', 'acoplar', 'hospedar', 'soñar', 'esconder', 'tostar', 'oir', 'inyectar', 'vomitar', 'persistir', 'hachar', 'verificar', 'transferir', 'controlar', 'graznar', 'piar', 'provocar', 'mirar', 'enfadar', 'recoger', 'trinar', 'votar', 'indagar', 'gastar', 'configurar', 'triunfar', 'sonreir', 'haber', 'amaron', 'descansar', 'rehacer', 'jinetear', 'malversar', 'opacar', 'habituar', 'poner', 'gestar', 'zarandear', 'victimizar', 'superar', 'babear', 'aterrar', 'ellas wasapeaban', 'justipreciar', 'lidiar', 'figurar', 'amaste', 'estudiar', 'gozar', 'poder', 'resbalar', 'intercambiar', 'citar', 'exprimir', 'facturar', 'ejercer', 'forrar', 'permitir', 'noctambular', 'intoxicar', 'capitular', 'contemplar', 'tapar', 'contratar', 'rezar', 'flamear', 'comparar', 'wasapearamos o wasapeasemos', 'comprimir', 'doler', 'servir', 'vaticinar', 'alunizar', 'bombardear', 'amaras', 'temblar', 'caldear', 'meter', 'proteger', 'describir', 'enviar', 'laminar', 'derribar', 'basar', 'exculpar', 'templar', 'sentir', 'congelar', 'correr', 'consistir', 'concientizar', 'asombrar', 'transmitir', 'ofertar', 'filmar', 'nuclear', 'rogar', 'wasapearas o wasapeases', 'charlar', 'legislar', 'relamer', 'retroceder', 'levitar', 'suplicar', 'empeñar', 'amara', 'crear', 'frotar', 'maquinar', 'cumplir', 'cortar', 'comportar', 'deducir', 'planchar', 'matricular', 'flexionar', 'caramelizar', 'modular', 'abonar', 'disfrutar', 'fraccionar', 'adoptar', 'afrontar', 'hisopar', 'detectar', 'polucionar', 'ladear', 'esquilar', 'borbotear', 'jurar', 'desviar', 'amo', 'exacerbar', 'insertar', 'guardar', 'nebulizar', 'experimentar', 'yerguen', 'soplar', 'han wasapeado', 'aman', 'hervir', 'cruzar', 'abordar', 'desquiciar', 'sustraer', 'idolatrar', 'ser', 'discutir', 'acudir', 'amemos', 'distribuyo', 'emprender', 'ojear', 'imitar', 'transformar', 'demoler', 'morder', 'musicalizar', 'sentar', 'navegar', 'interceder', 'dañar', 'coquetear', 'ningunear', 'expatriar', 'anticipar', 'barrer', 'engañar', 'guarecer', 'estresar', 'incomodar', 'homogeneizar', 'empobrecer', 'tropezar', 'validar', 'limitar', 'coser', 'wasapeabamos', 'argumentar', 'trocear', 'consentir', 'wasapeabais', 'censurar', 'zozobrar', 'refrigerar', 'disponer', 'jalar', 'wasapearas', 'enfervorizar', 'atrapar', 'entender', 'machacar', 'wasapeaste', 'heder', 'evolucionar', 'pasar', 'centralizar', 'hociquear', 'blanquear', 'parafrasear', 'homologar', 'sustituyeran', 'insistir', 'zoncear', 'zurrar', 'tripular', 'regular', 'tartamudear', 'apestar', 'refinar', 'manchar', 'mamar', 'vulnerar', 'bloquear', 'significar', 'insultar', 'cobrar', 'corregir', 'nausear', 'pagar', 'extorsionar', 'transcurrir', 'secar', 'unificar', 'wasapeaban', 'velar', 'contaminar', 'independizar']\n",
            "---------------------------------\n",
            "{'soy': 'ser', 'estuviste': 'estar', 'fuiste': 'ir', 'tuviste': 'tener', 'hiciste': 'hacer', 'dijiste': 'decir', 'dimar': 'decir', 'pudiste': 'poder', 'supiste': 'saber', 'pusiste': 'poner', 'viste': 'ver', 'diste': 'dar', 'damar': 'dar', 'viniste': 'venir', 'haya': 'haber', 'cupiste': 'caber', 'valiste': 'valer', 'quisiste': 'querer', 'llegaste': 'llegar', 'contaste': 'contar', 'cuesta': 'costar', 'duraste': 'durar', 'eres': 'ser', 'estas': 'estar', 'vas': 'ir', 'vaya': 'ir', 'tienes': 'tener', 'haces': 'hacer', 'dices': 'decir', 'dime': 'decir', 'puedes': 'poder', 'sabes': 'saber', 'pones': 'poner', 'ves': 'ver', 'das': 'dar', 'dame': 'dar', 'vienes': 'venir', 'has': 'haber', 'cabes': 'caber', 'vales': 'valer', 'quieres': 'querer', 'llegares': 'llegar', 'cuentas': 'contar', 'cuestan': 'costar', 'duro': 'durar', 'seras': 'ser', 'estaras': 'estar', 'iras': 'ir', 'tendras': 'tener', 'haras': 'hacer', 'diras': 'decir', 'digame': 'decir', 'podras': 'poder', 'sabras': 'saber', 'pondras': 'poner', 'veras': 'ver', 'daras': 'dar', 'vendras': 'venir', 'habras': 'haber', 'cabras': 'caber', 'valdras': 'valer', 'querras': 'querer', 'llegaras': 'llegar', 'contaras': 'contar', 'costo': 'costar', 'duraras': 'durar', 'eras': 'ser', 'estabas': 'estar', 'ibas': 'ir', 'tenias': 'tener', 'hacias': 'hacer', 'decias': 'decir', 'dimir': 'decir', 'podias': 'poder', 'sabias': 'saber', 'ponias': 'poner', 'veias': 'ver', 'dabas': 'dar', 'venias': 'venir', 'habias': 'haber', 'cabias': 'caber', 'valias': 'valer', 'querias': 'querer', 'llegarias': 'llegar', 'contabas': 'contar', 'costaria': 'costar', 'durabas': 'durar', 'es': 'ser', 'dimo': 'decir', 'darme': 'dar', 'hubiste': 'haber', 'cuentame': 'contar', 'costarian': 'costar', 'serias': 'ser', 'estarias': 'estar', 'irias': 'ir', 'tendrias': 'tener', 'harias': 'hacer', 'dirias': 'decir', 'dimiria': 'decir', 'podrias': 'poder', 'sabrias': 'saber', 'pondrias': 'poner', 'verias': 'ver', 'darias': 'dar', 'vendrias': 'venir', 'habrias': 'haber', 'cabrias': 'caber', 'valdrias': 'valer', 'querrias': 'querer', 'llegarrias': 'llegar', 'podria': 'poder', 'contarias': 'contar', 'cuestas': 'costar', 'durarias': 'durar'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Tratamiento de datos"
      ],
      "metadata": {
        "id": "kDRIGcy62JuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Función para encontrar la raiz de las palabras\n",
        "def raiz(palabra):\n",
        "    palabra_encontrada = palabra\n",
        "    max_similitud = 0.0\n",
        "\n",
        "    for verbo in lista_verbos:\n",
        "        similitud = jellyfish.jaro_similarity(palabra, verbo)\n",
        "        if similitud > max_similitud:\n",
        "            max_similitud = similitud\n",
        "            palabra_encontrada = verbo\n",
        "\n",
        "    if max_similitud >= 0.93:\n",
        "        return palabra_encontrada, max_similitud\n",
        "    else:\n",
        "        return palabra, max_similitud\n",
        "\n",
        "def tratamiento_texto(texto):\n",
        "  trans = str.maketrans('áéíóú','aeiou')\n",
        "  texto = texto.lower()\n",
        "  texto = texto.translate(trans)\n",
        "  texto = re.sub(r\"[^\\w\\s+\\-*/]\", '', texto)\n",
        "  texto = \" \".join(texto.split())\n",
        "  return texto\n",
        "\n",
        "#Función para reemplazar el final de una palabra por 'r'\n",
        "def reemplazar_terminacion(frase):\n",
        "    terminaciones = [\"es\", \"me\", \"as\", \"ste\", \"te\"]\n",
        "    palabras = frase.split()\n",
        "\n",
        "    for i, palabra in enumerate(palabras):\n",
        "        for terminacion in terminaciones:\n",
        "            if palabra.endswith(terminacion) and len(palabra) > len(terminacion):\n",
        "                palabras[i] = palabra[:-len(terminacion)] + \"r\"\n",
        "                break\n",
        "\n",
        "    nueva_frase = \" \".join(palabras)\n",
        "    return nueva_frase\n",
        "\n",
        "#Función para adicionar o eliminar tokens\n",
        "def revisar_tokens(texto, tokens):\n",
        "  #tokens: Es una lista vacia o no\n",
        "  #texto: Es la pregunta del usuario\n",
        "  if len(tokens)==0: #tokens está vacio, vamos a incluir tokens especiales\n",
        "    #Si encuentras alguna palabra compuesta especial en el texto adiciona su token:\n",
        "    #Si existe alguna: ['cientifico de datos', 'data scientist'] -> tokens.append('datascientist')\n",
        "    #Si existe alguna: ['ciencia de datos', 'data science'] -> tokens.append('datascience')\n",
        "    #Si existe alguna: ['elprofealejo', 'el profe alejo', 'profe alejo', 'profealejo'] -> tokens.append('elprofealejo')\n",
        "    # if [name for name in ['cientifico de datos', 'data scientist','ciencia de datos', 'data science','elprofealejo', 'el profe alejo', 'profe alejo', 'profealejo']if name in texto]:\n",
        "    if [name for name in ['cientifico de datos', 'data scientist'] if name in texto]: tokens.append('datascientist')\n",
        "    elif [name for name in ['ciencia de datos', 'data science'] if name in texto]: tokens.append('datascience')\n",
        "    elif [name for name in ['elprofealejo', 'el profe alejo', 'profe alejo', 'profealejo'] if name in texto]: tokens.append('elprofealejo')\n",
        "    # print(tokens)\n",
        "  else: #tokens no está vacio, vamos a eliminar tokens irrelevantes\n",
        "    elementos_a_eliminar = [\"cual\", \"que\", \"quien\", \"cuanto\", \"cuando\", \"como\"]\n",
        "    if 'hablame' in texto and 'hablar' in tokens: elementos_a_eliminar.append('hablar')\n",
        "    elif 'cuentame' in texto and 'contar' in tokens: elementos_a_eliminar.append('contar')\n",
        "    elif 'hago' in texto and 'hacer' in tokens: elementos_a_eliminar.append('hacer')\n",
        "    elif 'entiendes' in texto and 'entender' in tokens: elementos_a_eliminar.append('entender')\n",
        "    elif 'sabes' in texto and 'saber' in tokens: elementos_a_eliminar.append('saber')\n",
        "    #Ahora que tenemos nuestra blacklist(tokens irrelevantes) elimínalos de nuestra lista tokens\n",
        "    tokens = [token for token in tokens if token not in elementos_a_eliminar]\n",
        "    # print(tokens)\n",
        "  return tokens\n",
        "\n",
        "#Función para devolver los tokens normalizados del texto\n",
        "# def normalizar(texto):\n",
        "#   tokens=[]\n",
        "#   tokens=revisar_tokens(tratamiento_texto(texto), tokens)\n",
        "#   doc = nlp(texto)\n",
        "#   for t in doc:\n",
        "#     lemma=lista_verbos_irregulares.get(t.text, t.lemma_.split()[0])\n",
        "#     lemma=re.sub(r'[^\\w\\s+\\-*/]', '', lemma)\n",
        "#     if t.pos_ in ('VERB','PROPN','PRON','NOUN','AUX','SCONJ','ADJ','ADV','NUM') or lemma in lista_verbos:\n",
        "#       if t.pos_=='VERB':\n",
        "#         lemma = reemplazar_terminacion(lemma)\n",
        "#         tokens.append(raiz(tratamiento_texto(lemma)))\n",
        "#       else:\n",
        "#         tokens.append(tratamiento_texto(lemma))\n",
        "\n",
        "#   tokens = list(dict.fromkeys(tokens))\n",
        "#   tokens = list(filter(None, tokens))\n",
        "#   tokens = revisar_tokens(tratamiento_texto(texto), tokens)\n",
        "#   return str(tokens)\n",
        "\n",
        "def normalizar(texto):\n",
        "    tokens = []\n",
        "    tokens = revisar_tokens(tratamiento_texto(texto), tokens)\n",
        "    doc = nlp(texto)\n",
        "    for t in doc:\n",
        "        # Obtener el lemma\n",
        "        lemma = lista_verbos_irregulares.get(t.text, t.lemma_)\n",
        "\n",
        "        # Verificar si lemma es una cadena no vacía\n",
        "        if lemma and isinstance(lemma, str):\n",
        "            lemma = re.sub(r'[^\\w\\s+\\-*/]', '', lemma)\n",
        "\n",
        "            if t.pos_ in ('VERB','PROPN','PRON','NOUN','AUX','SCONJ','ADJ','ADV','NUM') or lemma in lista_verbos:\n",
        "                if t.pos_ == 'VERB':\n",
        "                    lemma = reemplazar_terminacion(lemma)\n",
        "                    tokens.append(raiz(tratamiento_texto(lemma)))\n",
        "                else:\n",
        "                    tokens.append(tratamiento_texto(lemma))\n",
        "\n",
        "    tokens = list(dict.fromkeys(tokens))\n",
        "    tokens = list(filter(None, tokens))\n",
        "    tokens = revisar_tokens(tratamiento_texto(texto), tokens)\n",
        "    tokens_str = str(tokens)\n",
        "    return tokens_str\n"
      ],
      "metadata": {
        "id": "N-__6F0I2MjG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Cargar bases de documentos"
      ],
      "metadata": {
        "id": "_RnqKO9ChOpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importando bases de dialogo fluído\n",
        "txt_folder_path = folder+'/dialogos'\n",
        "lista_documentos=[x for x in os.listdir(txt_folder_path) if x.endswith(\".txt\")]\n",
        "lista_dialogos, lista_dialogos_respuesta, lista_tipo_dialogo = [],[],[]\n",
        "for idx in range(len(lista_documentos)):\n",
        "  f=open(txt_folder_path+'/'+lista_documentos[idx], 'r', encoding='utf-8', errors='ignore')\n",
        "  estado = True\n",
        "  for line in f.read().split('\\n'):\n",
        "    if estado:\n",
        "      line_tratado = tratamiento_texto(line)\n",
        "      lista_dialogos.append(line_tratado)\n",
        "      lista_tipo_dialogo.append(lista_documentos[idx][:-4])\n",
        "    else:\n",
        "      lista_dialogos_respuesta.append(line)\n",
        "    estado=not estado\n",
        "\n",
        "#Creando Dataframe de diálogos\n",
        "datos = {'dialogo':lista_dialogos,'respuesta':lista_dialogos_respuesta,'tipo':lista_tipo_dialogo,'interseccion':0,'jaro_winkler':0,'probabilidad':0}\n",
        "df_dialogo = pd.DataFrame(datos)\n",
        "df_dialogo = df_dialogo.drop_duplicates(keep='first')\n",
        "df_dialogo.reset_index(drop=True, inplace=True)\n",
        "\n",
        "#Importando bases csv\n",
        "txt_folder_path = folder+'/documentos'\n",
        "lista_documentos=[x for x in os.listdir(txt_folder_path) if x.endswith(\".csv\")]\n",
        "documento_csv = ''\n",
        "for idx in range(len(lista_documentos)):\n",
        "  with open(txt_folder_path+'/'+lista_documentos[idx], \"r\", encoding=\"utf-8\") as archivo_csv:\n",
        "    lector_csv = csv.reader(archivo_csv)\n",
        "    for fila in lector_csv:\n",
        "      if fila[-1]!='frase':\n",
        "        documento_csv += fila[-1]\n",
        "\n",
        "#Importando bases docx\n",
        "lista_documentos=[x for x in os.listdir(txt_folder_path) if x.endswith(\".docx\")]\n",
        "documento_docx = ''\n",
        "for idx in range(len(lista_documentos)):\n",
        "  for paragraph in Document(txt_folder_path+'/'+lista_documentos[idx]).paragraphs:\n",
        "    documento_docx += paragraph.text.replace('*','\\n\\n*').replace('-','\\n-')\n",
        "\n",
        "#Importando bases txt\n",
        "lista_documentos=[x for x in os.listdir(txt_folder_path) if x.endswith(\".txt\")]\n",
        "documento_txt = ''\n",
        "for idx in range(len(lista_documentos)):\n",
        "  with open(txt_folder_path+'/'+lista_documentos[idx], \"r\", encoding=\"utf-8\") as archivo_txt:\n",
        "    lector_txt = archivo_txt.read()\n",
        "    for fila in lector_txt:\n",
        "      documento_txt += fila\n",
        "\n",
        "documento = documento_csv + documento_txt + documento_docx\n",
        "lista_frases = nltk.sent_tokenize(documento,'spanish')\n",
        "lista_frases_normalizadas = [' '.join(normalizar(x)) for x in lista_frases]"
      ],
      "metadata": {
        "id": "1894SyvPhQJ0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Guardar DF en drive:"
      ],
      "metadata": {
        "id": "CIRV2DyvtZPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def guardar_dataframe(dataframe):\n",
        "  carpeta = \"/content/drive/MyDrive/Chatbot\"\n",
        "  nombre_archivo = \"df_dialogo_spring_02.csv\"\n",
        "\n",
        "  if not os.path.exists(carpeta):\n",
        "    os.makedirs(carpeta)\n",
        "  archivo_csv = os.path.join(carpeta, nombre_archivo)\n",
        "  dataframe.to_csv(archivo_csv, index=False)"
      ],
      "metadata": {
        "id": "w5YqjPJ2jWJD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Buscar respuesta del Chatbot"
      ],
      "metadata": {
        "id": "IVJ6UmQV3n4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import jellyfish\n",
        "\n",
        "def dialogo(user_response):\n",
        "    # Tratamiento de texto\n",
        "    user_response = tratamiento_texto(user_response)  # Tratando el texto\n",
        "    user_response = re.sub(r\"[^\\w\\s]\", '', user_response)  # Elimina signos de puntuación\n",
        "\n",
        "    df = df_dialogo.copy()\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # Calcular la similitud de coseno usando TF-IDF\n",
        "        tfidf_matrix = tfidf_vectorizer.fit_transform([user_response, row['dialogo']])\n",
        "        cos_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
        "\n",
        "        df.at[idx, 'interseccion'] = len(set(user_response.split()) & set(row['dialogo'].split()))/len(user_response.split())\n",
        "        df.at[idx, 'similarity'] = cos_sim[0][0]\n",
        "        df.at[idx, 'jaro_winkler'] = jellyfish.jaro_winkler_similarity(user_response, row['dialogo'])\n",
        "\n",
        "        # Calcular la probabilidad:\n",
        "        df.at[idx, 'probabilidad'] = max(df.at[idx, 'interseccion'], df.at[idx, 'similarity'], df.at[idx, 'jaro_winkler'])\n",
        "\n",
        "    # Ordenar el DataFrame por probabilidad y Jaro-Winkler\n",
        "    df.sort_values(by=['probabilidad', 'jaro_winkler'], inplace=True, ascending=False)\n",
        "\n",
        "    # Obtener la probabilidad máxima\n",
        "    probabilidad = round(df['probabilidad'].head(1).values[0],2)\n",
        "\n",
        "    #Guardar df:\n",
        "    guardar_dataframe(df)\n",
        "\n",
        "    if probabilidad >= 0.93:\n",
        "        print('Respuesta encontrada por el método de comparación de textos - Probabilidad: ', probabilidad)\n",
        "        respuesta = df['respuesta'].head(1).values[0]\n",
        "    else:\n",
        "        respuesta = ''\n",
        "    return respuesta\n",
        "\n",
        "#Cargar tu modelo entrenado aqui(recuerda siempre cargar el modelo y el vectorizer o tokenizer usado en el entrenamiento del modelo):\n",
        "ruta_modelo = '/content/drive/MyDrive/Chatbot/'\n",
        "Modelo_TF = BertForSequenceClassification.from_pretrained(ruta_modelo)\n",
        "tokenizer_TF = BertTokenizer.from_pretrained(ruta_modelo)\n",
        "\n",
        "#Función para dialogar utilizando el modelo de Machine Learning\n",
        "def clasificacion_modelo(pregunta):\n",
        "  frase = normalizar(pregunta)\n",
        "  frase = ' '.join(str(elemento) for elemento in frase)\n",
        "  tokens = tokenizer_TF.encode_plus(\n",
        "      frase,\n",
        "      add_special_tokens=True,\n",
        "      max_length=128,\n",
        "      padding='max_length',\n",
        "      truncation=True,\n",
        "      return_tensors='pt'\n",
        "  )\n",
        "  input_ids = tokens['input_ids']\n",
        "  attention_mask = tokens['attention_mask']\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = Modelo_TF(input_ids, attention_mask)\n",
        "\n",
        "  etiquetas_predichas = torch.argmax(outputs.logits, dim=1)\n",
        "  etiquetas_decodificadas = etiquetas_predichas.tolist()\n",
        "\n",
        "  diccionario = {3: 'Continuacion', 10: 'Nombre', 2: 'Contacto', 13: 'Saludos', 14: 'Sentimiento', 9: 'Identidad', 15: 'Usuario', 6: 'ElProfeAlejo', 1: 'Aprendizaje', 0: 'Agradecimiento', 5: 'Edad', 4: 'Despedida', 11: 'Origen', 12: 'Otros', 7: 'Error', 8: 'Funcion'}\n",
        "  llave_buscada = etiquetas_decodificadas[0]\n",
        "  clase_encontrada = diccionario[llave_buscada]\n",
        "\n",
        "  #Buscar respuesta más parecida en la clase encontrada\n",
        "  df = df_dialogo[df_dialogo['tipo'] == clase_encontrada]\n",
        "  df.reset_index(inplace=True)\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  dialogos_num = vectorizer.fit_transform(df['dialogo'])\n",
        "  pregunta_num = vectorizer.transform([tratamiento_texto(pregunta)])\n",
        "  similarity_scores = cosine_similarity(dialogos_num, pregunta_num)\n",
        "  indice_pregunta_proxima = similarity_scores.argmax()\n",
        "\n",
        "  if max(similarity_scores)>0.5 and clase_encontrada not in ['Otros']:\n",
        "    print('Respuesta encontrada por el modelo Transformers - tipo:',clase_encontrada)\n",
        "    respuesta = df['respuesta'][indice_pregunta_proxima]\n",
        "  else:\n",
        "    respuesta = ''\n",
        "  return respuesta\n",
        "\n",
        "#Función para devolver la respuesta de los documentos\n",
        "def respuesta_documento(pregunta):\n",
        "  pregunta = normalizar(pregunta)\n",
        "  def contar_coincidencias(frase):\n",
        "    return sum(1 for elemento in pregunta if elemento in frase)\n",
        "\n",
        "  diccionario = {valor: posicion for posicion, valor in enumerate(lista_frases_normalizadas)}\n",
        "  lista = sorted(list(diccionario.keys()), key=contar_coincidencias, reverse=True)[:6]\n",
        "  #Hasta aqui ya tengo mi lista con las 6 respuestas con mayor coincidencia de tokens\n",
        "  #Convierte la pregunta en frase\n",
        "  #Adiciona la frase convertida al final de la lista\n",
        "  #Inicializa un TfidfVectorizer utilizando la función normalizar como tokenizer\n",
        "  #Entrenalo con fit_transform sobre la lista y asigna el resultado a la variable tfidf\n",
        "  if 'curso' not in pregunta: lista = [frase for frase in lista if 'curso' not in frase]\n",
        "  lista.append(' '.join(pregunta))\n",
        "  TfidfVec = TfidfVectorizer(tokenizer=normalizar)\n",
        "  tfidf = TfidfVec.fit_transform(lista)\n",
        "  #Aplica cosine_similarity entre el último elemento de tfidf y todos los elementos de tfidf y guárdalo en vals\n",
        "  vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "  idx = vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = round(flat[-2],2)\n",
        "  if req_tfidf>=0.22:\n",
        "    print('Respuesta encontrada por el método TfidfVectorizer - Probabilidad:', req_tfidf)\n",
        "    respuesta = lista_frases[diccionario[lista[idx]]]\n",
        "  else:\n",
        "    respuesta = ''\n",
        "  return respuesta\n",
        "\n",
        "#Función para devolver una respuesta final buscada en todos los métodos disponibles\n",
        "def respuesta_chatbot(pregunta):\n",
        "  respuesta = dialogo(pregunta)\n",
        "  if respuesta != '':\n",
        "    return respuesta\n",
        "  else:\n",
        "    respuesta = clasificacion_modelo(pregunta)\n",
        "    if respuesta != '':\n",
        "      return respuesta\n",
        "    else:\n",
        "      respuesta = respuesta_documento(pregunta)\n",
        "      if respuesta != '':\n",
        "        return respuesta\n",
        "      else:\n",
        "        return 'Respuesta no encontrada'"
      ],
      "metadata": {
        "id": "Bpxajyck3pav"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Ejecutar Chatbot"
      ],
      "metadata": {
        "id": "LMEwexpz4gdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pregunta='cursos ?'\n",
        "respuesta = respuesta_chatbot(pregunta)\n",
        "print(respuesta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_wq8HMP4hmU",
        "outputId": "a01d2ee1-7e7d-4b5c-dd09-d459cdae8755"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta encontrada por el método TfidfVectorizer - Probabilidad: 0.95\n",
            "El curso 'Fundamentos de Ciencia de Datos' tiene un tiempo de duración de 8 semanas, por un precio o costo de $500 USD y con fecha de inicio en 1 de julio de 2023.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}