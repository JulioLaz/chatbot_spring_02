{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9A2oSLDivrbN",
        "qQEIdACV2IE4",
        "kDRIGcy62JuE",
        "_RnqKO9ChOpY",
        "IVJ6UmQV3n4T"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JulioLaz/chatbot_spring_02/blob/main/Chatbot_Inteligente_Alura_Sprint2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Configurar ambiente"
      ],
      "metadata": {
        "id": "9A2oSLDivrbN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UD0OObFGtZVJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a11eb063-b714-4294-c347-8799888999bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-28 01:56:43.836643: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting es-core-news-md==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.6.0/es_core_news_md-3.6.0-py3-none-any.whl (42.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-md==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-md==3.6.0) (2.1.3)\n",
            "Installing collected packages: es-core-news-md\n",
            "Successfully installed es-core-news-md-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_md')\n",
            "Collecting jellyfish\n",
            "  Downloading jellyfish-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jellyfish\n",
            "Successfully installed jellyfish-1.0.1\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.3\n",
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.3)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184487 sha256=8d4c3a4f013bb5045d441ae33eb74f0e5e6752af358556ac9fceca7cd74ff16d\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/27/06/837436d4c3bd989b957a91679966f207bfd71d358d63a8194d\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Instalando bibliotecas\n",
        "import pandas as pd\n",
        "import re, os, random, pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import spacy\n",
        "!python -m spacy download es_core_news_md\n",
        "!pip install jellyfish\n",
        "import jellyfish\n",
        "!pip install transformers\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "!pip install python-docx\n",
        "from docx import Document\n",
        "import csv\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "#Definiendo variables del proyecto:\n",
        "nlp = spacy.load('es_core_news_md')\n",
        "\n",
        "#Conectando al Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder = '/content/drive/MyDrive/Chatbot'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Importar verbos"
      ],
      "metadata": {
        "id": "qQEIdACV2IE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "archivo_pickle_verbos = \"/content/drive/MyDrive/Chatbot/verbos/lista_verbos.pickle\"\n",
        "archivo_pickle_verbos_irregulares = \"/content/drive/MyDrive/Chatbot/verbos/verbos_irregulares.pickle\"\n",
        "\n",
        "# Importar la lista_verbos:\n",
        "with open(archivo_pickle_verbos, \"rb\") as verbos:\n",
        "        lista_verbos = pickle.load(verbos)\n",
        "\n",
        "# Importar el diccionario:\n",
        "with open(archivo_pickle_verbos_irregulares, \"rb\") as verbos_irregulares:\n",
        "        lista_verbos_irregulares = pickle.load(verbos_irregulares)\n",
        "\n",
        "\n",
        "print(lista_verbos)\n",
        "print('---------------------------------')\n",
        "print(lista_verbos_irregulares)"
      ],
      "metadata": {
        "id": "8DDVG_F22IvU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba11fc50-54d5-4941-d621-99f7fc5d58c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['parar', 'recomendar', 'cancelar', 'fanatizar', 'amaran o amasen', 'exponer', 'obedecer', 'quejar', 'echar', 'legitimar', 'perjudicar', 'organizar', 'molar', 'objetar', 'considerar', 'golear', 'mover', 'acertar', 'reunir', 'regir', 'ilusionar', 'simpatizar', 'conjeturar', 'helar', 'quitar', 'amariamos', 'destacar', 'llegar', 'sincronizar', 'lesionar', 'seducir', 'asistir', 'conservar', 'acordar', 'salvar', 'relucir', 'graduar', 'forzar', 'dar', 'deplorar', 'batear', 'mofar', 'estropear', 'aplastar', 'wasapeo', 'gestionar', 'suprimir', 'gruñir', 'progresar', 'suscribir', 'noticiar', 'cavar', 'alejar', 'galopar', 'virar', 'medir', 'actualizar', 'humanizar', 'convivir', 'gratificar', 'digerir', 'tocar', 'zonificar', 'amariais', 'aterrizar', 'hojear', 'cometer', 'sufrir', 'reciclar', 'obturar', 'divertir', 'ondear', 'listar', 'determinar', 'alentar', 'sumar', 'reflexionar', 'ames', 'anotar', 'mitificar', 'escribir', 'ilustrar', 'obtener', 'subir', 'socorrer', 'desprender', 'agregar', 'gandulear', 'improvisar', 'traquetear', 'idealizar', 'pescar', 'despertarse', 'decorar', 'ceñir', 'asar', 'balbucear', 'noquear', 'amaremos', 'desconectar', 'tambalear', 'vengar', 'bolear', 'trabar', 'resplandecer', 'vagar', 'pasmar', 'susurrar', 'amamos', 'suceder', 'pelar', 'toser', 'preocupar', 'martillar', 'zarpar', 'repartir', 'costar', 'llenar', 'contradecir', 'trasplantar', 'embrujar', 'exonerar', 'wasapearia', 'fabular', 'alquilar', 'capturar', 'granular', 'instruir', 'roer', 'ufanarse', 'filtrar', 'pulsar', 'proporcionar', 'fijar', 'fallar', 'identificar', 'mermar', 'agradar', 'arder', 'desmerecer', 'arquear', 'desquitar', 'contar', 'pesar', 'rendir', 'exfoliar', 'gritar', 'hartar', 'pegar', 'irrumpir', 'resaltar', 'lagrimear', 'filosofar', 'perdonar', 'defender', 'zaherir', 'corromper', 'girar', 'titular', 'guadañar', 'evadir', 'traspasar', 'homenajear', 'dominar', 'variar', 'disminuir', 'wasapea', 'querellar', 'tiritar', 'excavar', 'opinar', 'comentar', 'prohibir', 'repasar', 'vislumbrar', 'alegrar', 'fotografiar', 'mosquear', 'hechizar', 'nadar', 'hastiar', 'recluyen', 'galvanizar', 'planificar', 'fisurar', 'fluctuar', 'comenzar', 'recrudecer', 'exagerar', 'colonizar', 'calcular', 'exterminar', 'desayunar', 'lamer', 'fluir', 'invadir', 'encantar', 'combinar', 'apagar', 'televisar', 'probar', 'aconsejar', 'necesitar', 'disparar', 'manejar', 'oxidar', 'devenir', 'ostentar', 'bucear', 'bautizar', 'moler', 'reir', 'crujir', 'shockear', 'nosotros', 'empezar', 'compensar', 'estatizar', 'alumbrar', 'formalizar', 'imaginar', 'construir', 'disuadir', 'iluminar', 'condimentar', 'estrenar', 'originar', 'abastecer', 'prolongar', 'usurar', 'inhalar', 'declinar', 'ensuciar', 'humillar', 'examinar', 'exteriorizar', 'amare', 'gobernar', 'tramar', 'oscilar', 'huir', 'esperar', 'infringir', 'predecir', 'wasapearian', 'alimentar', 'creer', 'niñear', 'zaparrastrar', 'quebrar', 'wasapeareis', 'almorzar', 'expresar', 'asentir', 'resolver', 'bailar', 'chocar', 'deprimir', 'obsequiar', 'sobrar', 'fluye', 'arañar', 'redimir', 'justiciar', 'acusar', 'robar', 'amaramos o amasemos', 'quemar', 'urdir', 'enamorarse', 'ocultar', 'salir', 'fantasear', 'ovacionar', 'apoyar', 'construyo', 'pedalear', 'galantear', 'lacrar', 'wasapearais o wasapeaseis', 'wasapeen', 'disentir', 'envejecer', 'radiar', 'asfixiar', 'bromear', 'amar', 'masacrar', 'representar', 'proclamar', 'alterar', 'incluyeran', 'inscribir', 'aligerar', 'querer', 'vapulear', 'traer', 'heredar', 'tararear', 'oscurecer', 'disminuyen', 'nutrir', 'oficializar', 'guiar', 'traducir', 'escoger', 'descarrilar', 'colorear', 'luchar', 'wasapeamos', 'ama', 'resarcir', 'convidar', 'liar', 'narrar', 'desconocer', 'ameis', 'protagonizar', 'refrescar', 'doblar', 'montar', 'ensayar', 'comer', 'obrar', 'fruncir', 'marinar', 'subrayar', 'replicar', 'teclear', 'auxiliar', 'litigar', 'sobrevivir', 'obstruir', 'vibrar', 'hablar', 'embestir', 'merendar', 'refutar', 'dirigir', 'hacinar', 'aspirar', 'propagar', 'sobreseer', 'educar', 'centrar', 'extender', 'redistribuya', 'plantar', 'aprobar', 'practicar', 'alarmar', 'bordar', 'instrumentar', 'universalizar', 'desteñir', 'glosar', 'magnificar', 'urgir', 'procurar', 'rebatir', 'quedar', 'razonar', 'macanear', 'renacer', 'repeler', 'fundir', 'rugir', 'subyacer', 'datar', 'incidir', 'frenar', 'negociar', 'definir', 'estallar', 'preexistir', 'turnar', 'caber', 'tranquilizar', 'disolver', 'localizar', 'varar', 'refaccionar', 'chupar', 'programar', 'zigzaguear', 'lookear', 'concluir', 'grisear', 'dividir', 'violentar', 'brincar', 'alborotar', 'dictar', 'casar', 'linchar', 'tricotar', 'temer', 'novelar', 'kickear', 'urbanizar', 'partir', 'mensurar', 'diferenciar', 'emancipar', 'wasapeasteis', 'aburrir', 'sacudir', 'confundir', 'añorar', 'constituyeran', 'ignorar', 'judicializar', 'solicitar', 'nasalizar', 'escuchar', 'desistir', 'esquiar', 'orar', 'cayo', 'bracear', 'herrumbrar', 'wasapean', 'destituyeron', 'trastornar', 'blindar', 'adquirir', 'distribuir', 'balancear', 'wasapeariais', 'instituyo', 'disimular', 'oxigenar', 'testimoniar', 'entusiasmar', 'desmentir', 'fragmentar', 'exhortar', 'felicitar', 'condicionar', 'vacunar', 'encontrar', 'mudar', 'mostrar', 'afilar', 'armar', 'concluyeron', 'mancillar', 'explotar', 'bajar', 'lacerar', 'chatear', 'diseñar', 'valuar', 'mecer', 'adorar', 'exigir', 'birlar', 'suplir', 'prevenir', 'registrar', 'terminar', 'jugar', 'aguardar', 'obviar', 'ellos', 'salar', 'higienizar', 'gatear', 'jerarquizar', 'presentar', 'yerran', 'hibernar', 'venir', 'clavar', 'olvidar', 'gimotear', 'asquear', 'roncar', 'wasapeariamos', 'resultar', 'vaciar', 'languidecer', 'intuyo', 'laquear', 'comprometer', 'enganchar', 'enquistar', 'entrar', 'nominar', 'impregnar', 'mutilar', 'fusilar', 'paralizar', 'tramitar', 'latir', 'vociferar', 'andar', 'arrojar', 'espirar', 'redactar', 'hornear', 'adornar', 'wasapeabas', 'ordenar', 'habilitar', 'prender', 'comerciar', 'transcribir', 'saciar', 'comunicar', 'maldecir', 'conectar', 'juntar', 'anexar', 'impresionar', 'conducir', 'cerrar', 'murmurar', 'firmar', 'merecer', 'neutralizar', 'consolidar', 'obnubilar', 'germinar', 'activar', 'comprar', 'wasapearias', 'reservar', 'sudar', 'atribuyo', 'nacionalizar', 'exhibir', 'yacer', 'contribuyan', 'impedir', 'familiarizar', 'intuir', 'exorcizar', 'obstaculizar', 'flotar', 'amaria', 'pasear', 'humedecer', 'barajar', 'dudar', 'celebrar', 'ayudar', 'adelantar', 'sepultar', 'jubilar', 'depositar', 'preservar', 'disfrazar', 'admitir', 'abatir', 'escalar', 'numerar', 'golpear', 'eliminar', 'llorisquear', 'estar', 'unir', 'sobresalir', 'exceptuar', 'amasteis', 'completar', 'expirar', 'glasear', 'lijar', 'obstruyen', 'expulsar', 'ofender', 'finalizar', 'restaurar', 'pisar', 'patinar', 'cepillar', 'lapidar', 'balar', 'crackear', 'huyeron', 'vaporizar', 'fastidiar', 'suponer', 'diluye', 'ocasionar', 'soler', 'implementar', 'parir', 'hemos wasapeado', 'llamar', 'estimar', 'reportar', 'satirizar', 'aproximar', 'leer', 'diluir', 'brindar', 'engordar', 'restablecer', 'retrasar', 'haya', 'enfriar', 'acariciar', 'vedar', 'lexicalizar', 'vos', 'equivocar', 'declarar', 'arrastrar', 'leñar', 'desafinar', 'garabatear', 'cambiar', 'yendo', 'deber', 'ofrecer', 'afinar', 'mendigar', 'confesar', 'gasificar', 'torcer', 'enchufar', 'incluir', 'retribuir', 'has wasapeado', 'sabotear', 'emitir', 'pillar', 'husmear', 'aportar', 'presumir', 'noblecer', 'privatizar', 'gravitar', 'rehusar', 'sobreexcitar', 'conseguir', 'recluir', 'explicitar', 'levar', 'tasar', 'reprochar', 'referir', 'usurpar', 'coincidir', 'rascar', 'maquillarse', 'distinguir', 'acabar', 'recordar', 'portar', 'presionar', 'aquejar', 'excitar', 'detener', 'amarian', 'garantizar', 'imprimir', 'atender', 'indisponer', 'destruir', 'aliñar', 'reclamar', 'volar', 'codificar', 'enamorar', 'desnudar', 'encender', 'menear', 'ventilar', 'puntualizar', 'pulir', 'sorprender', 'existir', 'tronar', 'maniatar', 'pinchar', 'abolir', 'ocluir', 'climatizar', 'fundar', 'inspirar', 'sonorizar', 'legar', 'migrar', 'notar', 'excretar', 'adjuntar', 'tatuar', 'elevar', 'fingir', 'solucionar', 'pronunciar', 'procesar', 'ulcerar', 'militarizar', 'amaran', 'dibujar', 'vestir', 'reconstruyeron', 'personalizar', 'revertir', 'retirar', 'concentrar', 'amenizar', 'editar', 'hilar', 'administrar', 'ejecutar', 'financiar', 'divorciar', 'borrar', 'vincular', 'sumergir', 'respetar', 'generar', 'afluyen', 'nortear', 'hermanar', 'competir', 'curtir', 'diferir', 'taladrar', 'favorecer', 'preguntar', 'calmar', 'malgastar', 'niquelar', 'pensar', 'malcriar', 'vayamos', 'sucumbir', 'detestar', 'retribuyan', 'complicar', 'reducir', 'aprovechar', 'integrar', 'zamarrear', 'rockear', 'depender', 'tintar', 'advertir', 'rozar', 'acotar', 'recuperar', 'ganar', 'culpar', 'liberar', 'recopilar', 'preferir', 'lucir', 'vacilar', 'ha wasapeado', 'optimizar', 'remplazar', 'saltar', 'lamentar', 'enojar', 'accionar', 'confiar', 'gotear', 'tallar', 'apostar', 'regalar', 'delinquir', 'ungir', 'enseñar', 'decidir', 'reaccionar', 'multiplicar', 'romper', 'untar', 'trotar', 'reflejar', 'enumerar', 'circular', 'wasapee', 'exiliar', 'honrar', 'suspirar', 'guillotinar', 'leyera', 'zorrear', 'encoger', 'avisar', 'calentar', 'asegurar', 'barnizar', 'hincar', 'abandonar', 'enhebrar', 'baldear', 'vivir', 'marear', 'fotocopiar', 'burbujear', 'relacionar', 'remitir', 'orquestar', 'pintar', 'plantear', 'fortalecer', 'concebir', 'situar', 'dimitir', 'matar', 'radicalizar', 'inferir', 'sostener', 'soportar', 'nacer', 'destruyo', 'perfeccionar', 'nominalizar', 'besar', 'destinar', 'exhalar', 'glorificar', 'ahorrar', 'criminalizar', 'empujar', 'conocer', 'conquistar', 'atar', 'movilizar', 'restar', 'coronar', 'sanar', 'acomodar', 'inhabilitar', 'trasladar', 'atribuir', 'estimular', 'tratar', 'fabricar', 'hipotecar', 'degustar', 'bifurcar', 'señalar', 'pretender', 'acostar', 'apreciar', 'contactar', 'lubricar', 'inaugurar', 'planear', 'grabar', 'fregar', 'llevar', 'aplicar', 'equiparar', 'gemir', 'stalkear', 'ultimar', 'acceder', 'descender', 'reproducir', 'mantener', 'funcionar', 'rodar', 'cantar', 'proyectar', 'coordinar', 'rejuvenecer', 'fomentar', 'entretener', 'absorber', 'interferir', 'conceder', 'volver', 'inspeccionar', 'molestar', 'pender', 'oler', 'escupir', 'influye', 'magullar', 'seguir', 'trabajar', 'abrazar', 'invitar', 'revelar', 'inclinar', 'osar', 'orientar', 'bostezar', 'realizar', 'ablandar', 'omitir', 'penetrar', 'sospechar', 'sobrescribir', 'negar', 'limar', 'entrenar', 'volcar', 'pelear', 'elegir', 'persuadir', 'ingresar', 'explorar', 'quintuplicar', 'admirar', 'granizar', 'cuidar', 'invocar', 'castellanizar', 'causar', 'estilizar', 'ingerir', 'equilibrar', 'discriminar', 'informar', 'beneficiar', 'augurar', 'eyectar', 'he wasapeado', 'liderar', 'alegrarse', 'frustrar', 'instruye', 'inventar', 'picar', 'enterar', 'pronosticar', 'recibir', 'zampar', 'exclamar', 'aprender', 'ella', 'laborar', 'recelar', 'menospreciar', 'reconquistar', 'vilipendiar', 'malhumorar', 'hallar', 'maquillar', 'saber', 'gravar', 'aflojar', 'tomar', 'excluir', 'operar', 'descubrir', 'extenuar', 'tensar', 'purificar', 'anular', 'ejercitar', 'tragar', 'concurrir', 'ultrajar', 'asociar', 'relatar', 'sumir', 'consultar', 'expender', 'madurar', 'despedir', 'nivelar', 'visitar', 'guisar', 'usufructuar', 'zanjar', 'vallar', 'vejar', 'enriquecer', 'zafar', 'estremecer', 'recorrer', 'broncear', 'obsesionar', 'oprimir', 'escapar', 'mejorar', 'otear', 'impartir', 'halagar', 'buscar', 'cultivar', 'saquear', 'donar', 'perfumar', 'eximir', 'remover', 'wasapeemos', 'acelerar', 'enfurecer', 'tardar', 'prometer', 'clarear', 'ocurrir', 'bombear', 'respirar', 'jactar', 'torturar', 'veranear', 'transportar', 'presentir', 'elaborar', 'indexar', 'ironizar', 'vetar', 'boicotear', 'meditar', 'estirar', 'negrear', 'descongelar', 'atacar', 'verter', 'soltar', 'wasapeaba', 'jadear', 'fertilizar', 'inmiscuyan', 'tolerar', 'iniciar', 'enfatizar', 'gesticular', 'vagabundear', 'regatear', 'devorar', 'cuchichear', 'zancadillear', 'sindicalizar', 'acreditar', 'tumbar', 'encuestar', 'excluyen', 'botar', 'instalar', 'apuntar', 'subdividir', 'alojar', 'tener', 'otorgar', 'rehuyo', 'apurar', 'freir', 'pregonar', 'vencer', 'complejizar', 'nevar', 'entrometer', 'herrar', 'malograr', 'wasapearan', 'suspender', 'acampar', 'hundir', 'prestar', 'ubicar', 'deletrear', 'responder', 'poseer', 'deponer', 'resfriarse', 'zalear', 'maravillar', 'optar', 'interponer', 'endurecer', 'fortificar', 'desatar', 'irradiar', 'wasapeare', 'reinar', 'excomulgar', 'vendar', 'cifrar', 'eludir', 'stockear', 'consumir', 'granjear', 'titilar', 'flaquear', 'requerir', 'yuxtaponer', 'tentar', 'aliviar', 'extraer', 'explayar', 'inundar', 'retorcer', 'sujetar', 'pulverizar', 'contextualizar', 'habitar', 'utilizar', 'rebobinar', 'holgazanear', 'wasapeeis', 'llover', 'abrochar', 'abusar', 'seleccionar', 'aguantar', 'wasapeara o wasapease', 'observar', 'ovular', 'gerenciar', 'repetir', 'amenazar', 'deteriorar', 'cesar', 'condenar', 'trazar', 'bramar', 'formar', 'almacenar', 'guerrear', 'destituir', 'lastimar', 'errar', 'violar', 'besuquear', 'mezclar', 'modelar', 'obligar', 'amen', 'importar', 'holgar', 'wasapeais', 'izar', 'jabonar', 'exasperar', 'disputar', 'dormir', 'agarrar', 'debatir', 'masajear', 'acompañar', 'wasapeas', 'chasquear', 'zambullir', 'encajar', 'denigrar', 'jalonar', 'juerguear', 'restituir', 'clasificar', 'afirmar', 'amaras o amases', 'remar', 'tirar', 'ligar', 'incumplir', 'moldear', 'decir', 'ajustar', 'madrugar', 'simbolizar', 'batir', 'traficar', 'empequeñecer', 'asesorar', 'wasapearan o wasapeasen', 'tender', 'escabullir', 'entrevistar', 'aquietar', 'aturdir', 'criticar', 'notificar', 'formular', 'marginar', 'quebrantar', 'intimidar', 'engrasar', 'proceder', 'zapar', 'flexibilizar', 'apartar', 'corresponder', 'elogiar', 'juguetear', 'lloviznar', 'judicar', 'atemorizar', 'reprender', 'ver', 'publicar', 'agotar', 'vosotros', 'mentir', 'inhibir', 'explicar', 'guiñar', 'abortar', 'transpirar', 'intervenir', 'sacrificar', 'marchar', 'ir', 'ladrar', 'traicionar', 'reconstituyo', 'animar', 'asomar', 'librar', 'zurcir', 'esquivar', 'lavar', 'renunciar', 'disculpar', 'grajear', 'emplear', 'simular', 'rellenar', 'enfermar', 'igualar', 'combatir', 'embellecer', 'injuriar', 'anunciar', 'amareis', 'desear', 'perder', 'someter', 'apelar', 'investigar', 'cuajar', 'desplazar', 'establecer', 'confluyan', 'embaucar', 'señalizar', 'orinar', 'arrestar', 'fallecer', 'hacer', 'interrumpir', 'voltear', 'complacer', 'sintonizar', 'amarais o amaseis', 'bañar', 'sonar', 'horadar', 'grillar', 'tejer', 'ustedes', 'imponer', 'caminar', 'macerar', 'expandir', 'zapatear', 'rechazar', 'herir', 'asustar', 'posar', 'encargar', 'aclarar', 'platicar', 'sextuplicar', 'venerar', 'relajar', 'encerrar', 'repercutir', 'viajar', 'embrollar', 'amais', 'entristecer', 'morir', 'callar', 'vitorear', 'galardonar', 'subsistir', 'devolver', 'divisar', 'acoger', 'hurgar', 'manipular', 'mediar', 'entorpecer', 'exceder', 'anhelar', 'trepar', 'humear', 'adaptar', 'cocinar', 'amara o amase', 'gambetear', 'uniformar', 'estornudar', 'abrir', 'introducir', 'atrever', 'licuar', 'festejar', 'odiar', 'agitar', 'percibir', 'analizar', 'satisfacer', 'hidratar', 'copiar', 'unisonar', 'acortar', 'triturar', 'interpretar', 'legalizar', 'valorar', 'dejar', 'gustar', 'hurtar', 'usar', 'intentar', 'brotar', 'bambolear', 'machucar', 'faltar', 'ellas', 'narcotizar', 'justificar', 'lograr', 'coleccionar', 'burlar', 'lindar', 'brillar', 'aislar', 'indicar', 'aplaudir', 'cosechar', 'sacar', 'falsificar', 'hackear', 'telefonear', 'asumir', 'marchitar', 'malherir', 'banalizar', 'extinguir', 'desaconsejar', 'comprender', 'proveer', 'manifestar', 'parpadear', 'trackear', 'derretir', 'deshacer', 'arreglar', 'peinar', 'emigrar', 'jactarse', 'carecer', 'saludar', 'balear', 'fracasar', 'reparar', 'saborear', 'colaborar', 'cansar', 'durar', 'menguar', 'protestar', 'influir', 'noviar', 'resistir', 'enredar', 'ensanchar', 'separar', 'tornear', 'afeitar', 'trajinar', 'boxear', 'habeis', 'adivinar', 'revolver', 'naufragar', 'batallar', 'oyeramos', 'bufar', 'flirtear', 'hospitalizar', 'ulular', 'demostrar', 'sugerir', 'mencionar', 'patear', 'normalizar', 'acosar', 'surtir', 'jaquear', 'profetizar', 'recurrir', 'nombrar', 'martirizar', 'esterilizar', 'cenar', 'fusionar', 'infundir', 'hostigar', 'prostituya', 'aumentar', 'amarias', 'fiar', 'fascinar', 'incrementar', 'desarrollar', 'extrañar', 'rodear', 'distraer', 'hipnotizar', 'cooperar', 'constatar', 'regresar', 'convencer', 'ceder', 'hilvanar', 'temperar', 'compartir', 'añadir', 'confirmar', 'reponer', 'envolver', 'sintetizar', 'atrasar', 'olfatear', 'desvestir', 'vigilar', 'hamacar', 'malentender', 'contestar', 'esclarecer', 'llamear', 'compilar', 'mojar', 'facilitar', 'estacionar', 'resumir', 'pedir', 'atenuar', 'renovar', 'oponer', 'envidiar', 'inmigrar', 'reconocer', 'expiar', 'bastar', 'descomponer', 'sustituir', 'ocupar', 'licenciar', 'afectar', 'recitar', 'zaracear', 'maniobrar', 'readmitir', 'inmiscuir', 'nacarar', 'zunchar', 'motivar', 'invertir', 'asesinar', 'difundir', 'participar', 'juzgar', 'cautivar', 'aceptar', 'convertir', 'preparar', 'horrorizar', 'nublar', 'equivaler', 'llorar', 'visualizar', 'impulsar', 'exportar', 'importunar', 'zumbar', 'inquietar', 'esclavizar', 'cubrir', 'retener', 'entrañar', 'irrigar', 'privar', 'excusar', 'objetivar', 'evitar', 'cazar', 'proponer', 'enloquecer', 'hinchar', 'incorporar', 'beber', 'naturalizar', 'acostumbrar', 'lanzar', 'incurrir', 'luxar', 'exaltar', 'rasquetear', 'fumar', 'conversar', 'interesar', 'curar', 'wasapearemos', 'valer', 'ame', 'bonificar', 'wasapearon', 'levantar', 'limpiar', 'vender', 'restringir', 'revisar', 'amas', 'candar', 'decepcionar', 'manotear', 'wasapees', 'medicar', 'florecer', 'liquidar', 'acoplar', 'hospedar', 'soñar', 'esconder', 'tostar', 'oir', 'inyectar', 'vomitar', 'persistir', 'hachar', 'verificar', 'transferir', 'controlar', 'graznar', 'piar', 'provocar', 'mirar', 'enfadar', 'recoger', 'trinar', 'votar', 'indagar', 'gastar', 'configurar', 'triunfar', 'sonreir', 'haber', 'amaron', 'descansar', 'rehacer', 'jinetear', 'malversar', 'opacar', 'habituar', 'poner', 'gestar', 'zarandear', 'victimizar', 'superar', 'babear', 'aterrar', 'ellas wasapeaban', 'justipreciar', 'lidiar', 'figurar', 'amaste', 'estudiar', 'gozar', 'poder', 'resbalar', 'intercambiar', 'citar', 'exprimir', 'facturar', 'ejercer', 'forrar', 'permitir', 'noctambular', 'intoxicar', 'capitular', 'contemplar', 'tapar', 'contratar', 'rezar', 'flamear', 'comparar', 'wasapearamos o wasapeasemos', 'comprimir', 'doler', 'servir', 'vaticinar', 'alunizar', 'bombardear', 'amaras', 'temblar', 'caldear', 'meter', 'proteger', 'describir', 'enviar', 'laminar', 'derribar', 'basar', 'exculpar', 'templar', 'sentir', 'congelar', 'correr', 'consistir', 'concientizar', 'asombrar', 'transmitir', 'ofertar', 'filmar', 'nuclear', 'rogar', 'wasapearas o wasapeases', 'charlar', 'legislar', 'relamer', 'retroceder', 'levitar', 'suplicar', 'empeñar', 'amara', 'crear', 'frotar', 'maquinar', 'cumplir', 'cortar', 'comportar', 'deducir', 'planchar', 'matricular', 'flexionar', 'caramelizar', 'modular', 'abonar', 'disfrutar', 'fraccionar', 'adoptar', 'afrontar', 'hisopar', 'detectar', 'polucionar', 'ladear', 'esquilar', 'borbotear', 'jurar', 'desviar', 'amo', 'exacerbar', 'insertar', 'guardar', 'nebulizar', 'experimentar', 'yerguen', 'soplar', 'han wasapeado', 'aman', 'hervir', 'cruzar', 'abordar', 'desquiciar', 'sustraer', 'idolatrar', 'ser', 'discutir', 'acudir', 'amemos', 'distribuyo', 'emprender', 'ojear', 'imitar', 'transformar', 'demoler', 'morder', 'musicalizar', 'sentar', 'navegar', 'interceder', 'dañar', 'coquetear', 'ningunear', 'expatriar', 'anticipar', 'barrer', 'engañar', 'guarecer', 'estresar', 'incomodar', 'homogeneizar', 'empobrecer', 'tropezar', 'validar', 'limitar', 'coser', 'wasapeabamos', 'argumentar', 'trocear', 'consentir', 'wasapeabais', 'censurar', 'zozobrar', 'refrigerar', 'disponer', 'jalar', 'wasapearas', 'enfervorizar', 'atrapar', 'entender', 'machacar', 'wasapeaste', 'heder', 'evolucionar', 'pasar', 'centralizar', 'hociquear', 'blanquear', 'parafrasear', 'homologar', 'sustituyeran', 'insistir', 'zoncear', 'zurrar', 'tripular', 'regular', 'tartamudear', 'apestar', 'refinar', 'manchar', 'mamar', 'vulnerar', 'bloquear', 'significar', 'insultar', 'cobrar', 'corregir', 'nausear', 'pagar', 'extorsionar', 'transcurrir', 'secar', 'unificar', 'wasapeaban', 'velar', 'contaminar', 'independizar']\n",
            "---------------------------------\n",
            "{'soy': 'ser', 'estuviste': 'estar', 'fuiste': 'ir', 'tuviste': 'tener', 'hiciste': 'hacer', 'dijiste': 'decir', 'dimar': 'decir', 'pudiste': 'poder', 'supiste': 'saber', 'pusiste': 'poner', 'viste': 'ver', 'diste': 'dar', 'damar': 'dar', 'viniste': 'venir', 'haya': 'haber', 'cupiste': 'caber', 'valiste': 'valer', 'quisiste': 'querer', 'llegaste': 'llegar', 'contaste': 'contar', 'cuesta': 'costar', 'duraste': 'durar', 'eres': 'ser', 'estas': 'estar', 'vas': 'ir', 'vaya': 'ir', 'tienes': 'tener', 'haces': 'hacer', 'dices': 'decir', 'dime': 'decir', 'puedes': 'poder', 'sabes': 'saber', 'pones': 'poner', 'ves': 'ver', 'das': 'dar', 'dame': 'dar', 'vienes': 'venir', 'has': 'haber', 'cabes': 'caber', 'vales': 'valer', 'quieres': 'querer', 'llegares': 'llegar', 'cuentas': 'contar', 'cuestan': 'costar', 'duro': 'durar', 'seras': 'ser', 'estaras': 'estar', 'iras': 'ir', 'tendras': 'tener', 'haras': 'hacer', 'diras': 'decir', 'digame': 'decir', 'podras': 'poder', 'sabras': 'saber', 'pondras': 'poner', 'veras': 'ver', 'daras': 'dar', 'vendras': 'venir', 'habras': 'haber', 'cabras': 'caber', 'valdras': 'valer', 'querras': 'querer', 'llegaras': 'llegar', 'contaras': 'contar', 'costo': 'costar', 'duraras': 'durar', 'eras': 'ser', 'estabas': 'estar', 'ibas': 'ir', 'tenias': 'tener', 'hacias': 'hacer', 'decias': 'decir', 'dimir': 'decir', 'podias': 'poder', 'sabias': 'saber', 'ponias': 'poner', 'veias': 'ver', 'dabas': 'dar', 'venias': 'venir', 'habias': 'haber', 'cabias': 'caber', 'valias': 'valer', 'querias': 'querer', 'llegarias': 'llegar', 'contabas': 'contar', 'costaria': 'costar', 'durabas': 'durar', 'es': 'ser', 'dimo': 'decir', 'darme': 'dar', 'hubiste': 'haber', 'cuentame': 'contar', 'costarian': 'costar', 'serias': 'ser', 'estarias': 'estar', 'irias': 'ir', 'tendrias': 'tener', 'harias': 'hacer', 'dirias': 'decir', 'dimiria': 'decir', 'podrias': 'poder', 'sabrias': 'saber', 'pondrias': 'poner', 'verias': 'ver', 'darias': 'dar', 'vendrias': 'venir', 'habrias': 'haber', 'cabrias': 'caber', 'valdrias': 'valer', 'querrias': 'querer', 'llegarrias': 'llegar', 'podria': 'poder', 'contarias': 'contar', 'cuestas': 'costar', 'durarias': 'durar'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Tratamiento de datos"
      ],
      "metadata": {
        "id": "kDRIGcy62JuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Función para encontrar la raiz de las palabras\n",
        "def raiz(palabra):\n",
        "    max_similitud = 0.0\n",
        "    for verbo in lista_verbos:\n",
        "        similitud = jellyfish.jaro_similarity(palabra, verbo)\n",
        "        if similitud > max_similitud:\n",
        "            max_similitud = similitud\n",
        "            palabra_encontrada = verbo\n",
        "\n",
        "    if max_similitud >= 0.93:\n",
        "        return palabra_encontrada, max_similitud\n",
        "    else:\n",
        "        return palabra, max_similitud\n",
        "\n",
        "def tratamiento_texto(texto):\n",
        "  trans = str.maketrans('áéíóú','aeiou')\n",
        "  texto = texto.lower()\n",
        "  texto = texto.translate(trans)\n",
        "  texto = re.sub(r\"[^\\w\\s+\\-*/]\", '', texto)\n",
        "  texto = \" \".join(texto.split())\n",
        "  return texto\n",
        "\n",
        "#Función para reemplazar el final de una palabra por 'r'\n",
        "def reemplazar_terminacion(frase):\n",
        "    terminaciones = [\"es\", \"me\", \"as\", \"ste\", \"te\"]\n",
        "    palabras = frase.split()\n",
        "\n",
        "    for i, palabra in enumerate(palabras):\n",
        "        for terminacion in terminaciones:\n",
        "            if palabra.endswith(terminacion) and len(palabra) > len(terminacion):\n",
        "                palabras[i] = palabra[:-len(terminacion)] + \"r\"\n",
        "                break\n",
        "\n",
        "    nueva_frase = \" \".join(palabras)\n",
        "    return nueva_frase\n",
        "\n",
        "#Función para adicionar o eliminar tokens\n",
        "def revisar_tokens(texto, tokens):\n",
        "  texto=tratamiento_texto(texto)\n",
        "  #tokens: Es una lista vacia o no\n",
        "  #texto: Es la pregunta del usuario\n",
        "  if len(tokens)==0: #tokens está vacio, vamos a incluir tokens especiales\n",
        "    #Si encuentras alguna palabra compuesta especial en el texto adiciona su token:\n",
        "    #Si existe alguna: ['cientifico de datos', 'data scientist'] -> tokens.append('datascientist')\n",
        "    #Si existe alguna: ['ciencia de datos', 'data science'] -> tokens.append('datascience')\n",
        "    #Si existe alguna: ['elprofealejo', 'el profe alejo', 'profe alejo', 'profealejo'] -> tokens.append('elprofealejo')\n",
        "\n",
        "    if any(name in texto for name in ['cientifico de datos', 'data scientist']):\n",
        "        tokens.append('datascientist')\n",
        "    if any(name in texto for name in ['elprofealejo', 'el profe alejo', 'profe alejo', 'profealejo']):\n",
        "        tokens.append('elprofealejo')\n",
        "    if any(name in texto for name in ['ciencia de datos', 'data science']):\n",
        "        tokens.append('datascience')\n",
        "    # print('tokens: ', tokens)\n",
        "  else: #tokens no está vacio, vamos a eliminar tokens irrelevantes\n",
        "    elementos_a_eliminar = [\"cual\", \"que\", \"quien\", \"cuanto\", \"cuando\", \"como\"]\n",
        "    if 'hablame' in texto and 'hablar' in tokens: elementos_a_eliminar.append('hablar')\n",
        "    elif 'cuentame' in texto and 'contar' in tokens: elementos_a_eliminar.append('contar')\n",
        "    elif 'hago' in texto and 'hacer' in tokens: elementos_a_eliminar.append('hacer')\n",
        "    elif 'entiendes' in texto and 'entender' in tokens: elementos_a_eliminar.append('entender')\n",
        "    elif 'sabes' in texto and 'saber' in tokens: elementos_a_eliminar.append('saber')\n",
        "    #Ahora que tenemos nuestra blacklist(tokens irrelevantes) elimínalos de nuestra lista tokens\n",
        "    tokens = [token for token in tokens if token not in elementos_a_eliminar]\n",
        "    # print('elementos_a_eliminar: ',elementos_a_eliminar)\n",
        "    # print(tokens)\n",
        "  return tokens\n",
        "\n",
        "#Función para devolver los tokens normalizados del texto\n",
        "def normalizar(texto):\n",
        "    tokens = []\n",
        "    tokens = revisar_tokens(texto, tokens)\n",
        "    doc = nlp(texto)\n",
        "    for t in doc:\n",
        "        # Obtener el lemma\n",
        "        lemma = lista_verbos_irregulares.get(t.text, t.lemma_)\n",
        "        # print('lemma001: ',lemma)\n",
        "        # lemma=lista_verbos_irregulares.get(t.text, t.lemma_.split()[0])\n",
        "\n",
        "        # Verificar si lemma es una cadena no vacía\n",
        "        if lemma and isinstance(lemma, str):\n",
        "            lemma = re.sub(r'[^\\w\\s+\\-*/]', '', lemma)\n",
        "\n",
        "            if t.pos_ in ('VERB','PROPN','PRON','NOUN','AUX','SCONJ','ADJ','ADV','NUM') or lemma in lista_verbos:\n",
        "                if t.pos_ == 'VERB':\n",
        "                    # print('lemma002: ',lemma)\n",
        "                    lemma = reemplazar_terminacion(lemma)\n",
        "                    # print('lemma003: ',lemma)\n",
        "                    tokens.append(raiz(tratamiento_texto(lemma)))\n",
        "                    # print('tokens: ',tokens)\n",
        "                else:\n",
        "                    tokens.append(tratamiento_texto(lemma))\n",
        "\n",
        "    tokens = list(dict.fromkeys(tokens))\n",
        "    tokens = list(filter(None, tokens))\n",
        "    tokens = revisar_tokens(texto, tokens)\n",
        "    tokens_str = str(tokens)\n",
        "    return tokens_str\n"
      ],
      "metadata": {
        "id": "N-__6F0I2MjG"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalizar(\"profe alejo cientifico de datos\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "8sMDNXp3QHj3",
        "outputId": "6f659266-de0f-42bd-baea-b3259ff25ece"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"['datascientist', 'elprofealejo', 'profe', ('alejo', 0.8222222222222223), 'cientifico', 'dato']\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Cargar bases de documentos"
      ],
      "metadata": {
        "id": "_RnqKO9ChOpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importando bases de dialogo fluído\n",
        "txt_folder_path = folder+'/dialogos'\n",
        "lista_documentos=[x for x in os.listdir(txt_folder_path) if x.endswith(\".txt\")]\n",
        "lista_dialogos, lista_dialogos_respuesta, lista_tipo_dialogo = [],[],[]\n",
        "for idx in range(len(lista_documentos)):\n",
        "  f=open(txt_folder_path+'/'+lista_documentos[idx], 'r', encoding='utf-8', errors='ignore')\n",
        "  estado = True\n",
        "  for line in f.read().split('\\n'):\n",
        "    if estado:\n",
        "      line_tratado = tratamiento_texto(line)\n",
        "      lista_dialogos.append(line_tratado)\n",
        "      lista_tipo_dialogo.append(lista_documentos[idx][:-4])\n",
        "    else:\n",
        "      lista_dialogos_respuesta.append(line)\n",
        "    estado=not estado\n",
        "\n",
        "#Creando Dataframe de diálogos\n",
        "datos = {'dialogo':lista_dialogos,'respuesta':lista_dialogos_respuesta,'tipo':lista_tipo_dialogo,'interseccion':0,'jaro_winkler':0,'probabilidad':0}\n",
        "df_dialogo = pd.DataFrame(datos)\n",
        "df_dialogo = df_dialogo.drop_duplicates(keep='first')\n",
        "df_dialogo.reset_index(drop=True, inplace=True)\n",
        "\n",
        "#Importando bases csv\n",
        "txt_folder_path = folder+'/documentos'\n",
        "lista_documentos=[x for x in os.listdir(txt_folder_path) if x.endswith(\".csv\")]\n",
        "documento_csv = ''\n",
        "for i in range(len(lista_documentos)):\n",
        "  with open(txt_folder_path+'/'+lista_documentos[i], \"r\", encoding=\"utf-8\") as csv_txt:\n",
        "    csv_text = csv.reader(csv_txt)\n",
        "    for fila in csv_text:\n",
        "      if fila[-1]!='frase':\n",
        "        documento_csv += fila[-1]\n",
        "\n",
        "#Importando bases docx\n",
        "lista_documentos=[x for x in os.listdir(txt_folder_path) if x.endswith(\".docx\")]\n",
        "documento_docx = ''\n",
        "for i in range(len(lista_documentos)):\n",
        "  for t in Document(txt_folder_path+'/'+lista_documentos[i]).paragraphs:\n",
        "    documento_docx += t.text.replace('*','\\n\\n*').replace('-','\\n-')\n",
        "\n",
        "#Importando bases txt\n",
        "lista_documentos=[x for x in os.listdir(txt_folder_path) if x.endswith(\".txt\")]\n",
        "documento_txt = ''\n",
        "for i in range(len(lista_documentos)):\n",
        "  with open(txt_folder_path+'/'+lista_documentos[i], \"r\", encoding=\"utf-8\") as txt:\n",
        "    txt_new = txt.read()\n",
        "    for i in txt_new:\n",
        "      documento_txt += i\n",
        "# print('documento_csv: ',documento_csv)\n",
        "# print('------------------------------------------------------------------------------')\n",
        "# print('documento_docx: ', documento_docx)\n",
        "# print('------------------------------------------------------------------------------')\n",
        "# print('documento_txt: ', documento_txt)\n"
      ],
      "metadata": {
        "id": "1894SyvPhQJ0"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# documento = documento_csv + documento_txt + documento_docx\n",
        "documento = documento_txt+documento_docx+documento_csv\n",
        "lista_frases = nltk.sent_tokenize(documento,'spanish')\n",
        "lista_frases_normalizadas = [''.join(str(normalizar(x))) for x in lista_frases]"
      ],
      "metadata": {
        "id": "Qc_ciUZTVKrp"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lista_frases)\n",
        "print(lista_frases_normalizadas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhlCjtTEKiWL",
        "outputId": "ae5574eb-5a04-417f-ce34-ca5a00432fe1"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['La ciencia de datos es un campo académico interdisciplinario que utiliza estadística, computación científica, métodos, procesos, algoritmos y sistemas científicos para obtener (recolectar o extraer), tratar, analizar y presentar informes a partir de datos ruidosos, estructurados y no estructurados.', 'La ciencia de datos es multifacética y puede describirse como una ciencia, un paradigma de investigación, un método de investigación, una disciplina, un flujo de trabajo o una profesión.', 'La ciencia de datos integra el conocimiento del dominio de la aplicación subyacente (por ejemplo, ciencia económica, finanzas, medicina, ciencias naturales, tecnologías de la información) y es un \"concepto consistente en unificar estadística, análisis de datos, informática y sus métodos relacionados\" para \"comprender y analizar fenómenos reales\" con datos.', 'La ciencia de datos utiliza métodos, técnicas y teorías extraídas de muchos campos dentro del contexto de las matemáticas, las estadísticas, las ciencias de la computación, las ciencias de la información y el conocimiento del dominio.', 'La ciencia de datos es diferente de la informática, la estadística y la ciencia de la información, el ganador del premio Turing, Jim Gray, imaginó la ciencia de datos como un \"cuarto paradigma\" de la ciencia (empírico, teórico, computacional y ahora basado en datos) y afirmó que \"todo sobre la ciencia está cambiando debido al impacto de la tecnología de la información\" y la avalancha de datos.', 'Un científico de datos es el profesional que mediante la escritura y aplicación de código de programación y conocimientos en estadística trabaja en la recolección de datos, la limpieza de datos, la exploración de datos, la modelación de datos, visualización de datos, la implementación de soluciones de aprendizaje automático y en la interpretación de resultados.', 'El perfil de un científico de datos proviene de diferentes profesiones o backgrounds: matemáticos, ingenieros, economistas, actuarios, físicos, químicos, y en algunas ocasiones de campos que pudieran parecer muy distantes como la medicina o la sociología.', 'En 1962, John W Tukey mencionó el término “Ciencia de Datos” en su artículo “The Future of Data Analysis” al explicar una evolución de la estadística matemática, en este, definió por primera vez el análisis de datos como: “Procedimientos para analizar datos, técnicas para interpretar los resultados de dichos procedimientos, formas de planificar la recopilación de datos para hacer su análisis más fácil, más preciso o acertado, y toda la maquinaria y los resultados de las estadísticas matemáticas que se aplican al análisis de datos\".', 'En 1977, John W Tukey publicó “Exploratory Data Analysis”, argumentando que era necesario poner más énfasis en el uso de datos para sugerir hipótesis que probar en modelos estadísticos.', 'La ciencia de datos ha resultado para muchos una disciplina de reciente creación, pero en la realidad este concepto lo utilizó por primera vez el científico danés Peter Naur en la década de los sesenta como sustituto de las ciencias computacionales.', 'En 1974 Peter Naur publicó el libro Concise Survey of Computer Methods, donde utiliza ampliamente el concepto ciencia de datos, lo que permitió una utilización más libre en el mundo académico.', 'En 1977, el International Association for Statistical Computing (IASC) es establecido como una sección del International Statistical Institute (ISI), “Es la misión de la IASC relacionar la metodología estadística tradicional, tecnología computacional moderna, y el conocimiento de expertos del tema, para convertir datos en información y conocimiento\".', 'En 1996 el término ‘Ciencia de Datos’ fue utilizado por primera vez en una conferencia llamada \"Ciencia de datos, clasificación y métodos relacionados\", que tuvo lugar en una reunión de miembros de la ‘International Federation of Classification Societies’ (IFCS) con sede en Kobe, Japón.', 'En 1997, Jeff Wu describió al trabajo estadístico como una trilogía conformada por recolección de datos, análisis y modelado de datos, y la toma de decisiones, haciendo la petición de que la estadística fuese renombrada como ciencia de datos, y los estadísticos como científicos de datos.', 'En 2001, William S Cleveland introdujo a la ciencia de datos como una disciplina independiente, extendiendo el campo de la estadística para incluir los avances en computación con datos en su artículo \"Data science: an action plan for expanding the technical areas of the field of statistics\", Cleveland estableció seis áreas técnicas que en su opinión conformarían al campo de la ciencia de datos: investigaciones multidisciplinarias, modelos y métodos para datos, computación con datos, pedagogía, evaluación de herramientas, y teoría.', 'En abril del 2002, el ‘International Council for Science: Committee on Data for Science and Technology’ (CODATA) empezó la publicación del Data Science Journal, enfocada en problemas como la descripción de sistemas de datos, su publicación en Internet, sus aplicaciones, y sus problemas legales, poco después, en enero del 2003, la Universidad de Columbia empezó a publicar The Journal of Data Science, la cual ofreció una plataforma para que todos los profesionales de datos presentaran sus perspectivas e intercambiaran ideas.', 'En 2005, The National Science Board publicó \"Long-Lived Digital Data Collections Enabling Research and Education in the 21st Century\", definiendo a los científicos de datos como \"científicos de computación e información, programadores de bases de datos y software, y expertos disciplinarios, que son cruciales para la gestión exitosa de una colección digital de datos, cuya actividad primaria es realizar investigación creativa y análisis\", fue en el 2008 que Jeff Hammerbacher y DJ Patil lo reutilizaron para definir sus propios trabajos realizados en Facebook y LinkedIn, respectivamente.', 'En 2009, los investigadores Yangyong Zhu y Yun Xiong del ‘Research Center for Dataology and Data Science’, publicaron “Introduction to Dataology and Data Science”, en donde manifiestan que “a diferencia de las ciencias naturales y las ciencias sociales, Datología y Ciencia de Datos toman datos en la red y su objeto de estudio” \\n\\nEn 2013 fue lanzado el ‘IEEE Task Force on Data Science and Advanced Analytics’, mientras que la primera conferencia internacional de ‘IEEE International Conference on Data Science and Advanced Analytics’ fue lanzada en el 2014.', 'En 2015, el International Journal on Data Science and Analytics fue lanzado por Springer para publicar trabajos originales en ciencia de datos y analítica de big data.', 'Sobre el roadmap para convertirse en científico de datos, existen diferentes formas de adquirir el conocimiento necesario, las universidades están empezando a ofrecer cursos y diplomados y algunas, maestrías y doctorados en ciencia de datos.', 'IBM Network Skills ofrece un Certificado Profesional en Ciencia de Datos que tiene un costo (pero se puede pedir ayuda económica para cursarlo gratuitamente), el cual está compuesto por 9 cursos y tiene una duración de 10 meses, más información en https://www.coursera.org/professional-certificates/certificado-profesional-de-ciencia-de-datos-de-ibm.', 'La Universidad Peruana de Ciencias Aplicadas a través de su Escuela de Postgrado también ofrece Cursos de Ciencia de Datos, son cursos cortos en la modalidad online y virtual distribuidos en sus categorías Flex Courses, de 6 horas académicas, y Cursos Especializados, de 24 horas.', 'Sobre ejemplos de aplicaciones de Ciencia de Datos en el Marketing, en septiembre de 1994, BusinessWeek publicó el artículo “Marketing de base de datos”, manifestando que las empresas recopilan una gran cantidad de información sobre los clientes, la cual es analizada para predecir la probabilidad de que compre un producto, afirman que se utiliza ese conocimiento para elaborar un mensaje de marketing calibrado con precisión para que el individuo busque conseguirlo, asimismo, explican que, en los ochenta, un entusiasmo provocado por la propagación de los lectores de códigos de barras terminó en una decepción generalizada pues muchas empresas fueron abrumadas por la gran cantidad de datos para lograr hacer algo útil con la información de sus clientes, sin embargo, muchas empresas creen que no hay más remedio que desafiar la frontera marketing y bases de datos para desarrollar más las tecnologías necesarias.', 'Sobre ejemplos de aplicaciones de Ciencia de Datos en el Marketing, en 2014 la empresa sueca de música en streaming Spotify compra The Echo Nest, una compañía especializada en ciencia de datos musicales, ésta ahora es la encargada de almacenar y analizar la información de sus 170 millones de usuarios, con ayuda de dicha empresa, en 2015 Spotify lanzó un servicio de música personalizada llamado Discover Weekly que semanalmente recomienda a sus usuarios una selección de canciones que podría interesarles por medio de algoritmos y análisis de los datos de la música escuchada y el historial de búsqueda de la semana pasada, el servicio recibió una buena recepción generalizada y actualmente figura un fuerte punto de venta ante la competencia de la empresa.', 'Sobre ejemplos de aplicaciones de Ciencia de Datos en el Marketing, Netflix, la empresa norteamericana de contenido multimedia en streaming ofrece a sus más de 120 millones de usuarios una plataforma capaz de analizar, mediante algoritmos, las costumbres de consumo de los usuarios para diferenciar los contenidos que estos buscan y lograr determinar qué nuevos contenidos les pueden interesar, Todd Yellin, vicepresidente de producto en Netflix, explicó que algunos de los datos almacenados pueden extenderse desde la hora del día se conectan sus usuarios, cuánto tiempo pasan dentro de la plataforma, su lista de contenidos recientemente vistos (para analizar incluso el orden específico de estos), toda la información que se almacena es utilizada específicamente para ser analizada, aprender del usuario y poder darle recomendaciones acertadas.', 'Sobre ejemplos de aplicaciones de Ciencia de Datos en Gobernanza, en América Latina el Banco Interamericano de Desarrollo (BID) ha desarrollado estudios exploratorios en los que se analiza la ciencia de datos en la implementación y diseño de políticas públicas en la región, tomando casos en países como Argentina y Brasil, presentando recomendaciones para su implementación y mantenimiento, éstas van desde temas como movilidad urbana sostenible, ciudades inteligentes, seguridad, propiedad de datos y privacidad, entre las sugerencias presentadas en las investigaciones está la de lograr una “inteligencia del valor público, la cual “tiene la potencialidad de ser un componente estratégico para la toma de decisiones y el diseño, implementación y evaluación de políticas públicas”, otra de ellas es la capacidad para lograr desde este campo una mejora de rendición de cuentas de los gobiernos ante la ciudadanía y promover un avance en cuanto a la curaduría de datos en las instituciones públicas.', 'Textualmente, Big Data se refiere a enormes volúmenes de datos que no pueden procesarse de manera efectiva con las aplicaciones tradicionales que actualmente se aplican, de acuerdo con la guía de Amazon Web Service, se considera al Big Data como una colección considerable de datos con dificultades para almacenarse en bases de datos tradicionales, y también para procesarse en servidores estándar y para analizarse con aplicaciones habituales.', 'Big Data se suele relacionar con ciencia de datos, pues esa suele ser su fuente de información para análisis.', 'La ciencia de datos logra trabajar y analizar los grandes conjuntos de datos desordenados e incompletos, para llegar a hallazgos que impulsan decisiones sobre operaciones y productos.', 'Se define al científico de datos como una mezcla de estadísticos, computólogos y pensadores creativos, con las siguientes habilidades: \\n\\n- Recopilar, procesar y extraer valor de las diversas y extensas bases de datos\\n- Imaginación para comprender, visualizar y comunicar sus conclusiones a los no científicos de datos\\n- Capacidad para crear soluciones basadas en datos que aumentan los beneficios, reducen los costos.', 'El proceso que sigue un científico de datos para trabajar o resolver problemas con los datos se puede resumir en estos pasos: \\n\\n- Extraer datos, independientemente de la fuente y de su volumen\\n- Limpiar los datos, para eliminar lo que pueda sesgar los resultados\\n- Procesar los datos usando métodos estadísticos como inferencia estadística, modelos de regresión, pruebas de hipótesis, etc\\n- Diseñar experimentos adicionales en caso de ser necesario\\n- Crear visualizaciones gráficas de los datos relevantes de la investigación.', 'El científico de datos es un estadístico que debería saber o aprender interfaces de programación de aplicaciones (API), bases de datos y extracción de datos; es un diseñador que deberá aprender a programar; y es un computólogo que deberá saber analizar y encontrar datos con significado.', 'En la tesis doctoral de Benjamin Fry explicó que el proceso para comprender mejor a los datos comenzaba con una serie de números y el objetivo de responder preguntas sobre los datos, en cada fase del proceso que él propone (adquirir, analizar, filtrar, extraer, representar, refinar e interactuar), se requiere de diferentes enfoques especializados que aporten a una mejor comprensión de los datos, entre los enfoques que menciona Fry están: ingenieros en sistemas, matemáticos, estadísticos, diseñadores gráficos, especialistas en visualización de la información y especialistas en interacciones hombre-máquina, mejor conocidos por sus siglas en inglés “HCI” (Human-Computer Interaction), además, Fry afirmó que contar con diferentes enfoques especializados lejos de resolver el problema de entendimiento de datos, se convierte en parte del problema, ya que cada especialización conduce de manera aislada el problema y el camino hacia la solución se puede perder algo en cada transición del proceso.', 'Drew Conway en su página web explica con la ayuda de un diagrama de Venn, las principales habilidades que le dan vida y forma a la ciencia de datos, así como sus relaciones de conjuntos.', 'La ciencia de datos ha cobrado recientemente mucha importancia en nuestro acontecer como disciplina o profesión emergente (científico de datos), y se ha vuelto en foco de atención de cada vez más organizaciones a nivel mundial, tal como lo señaló el economista en jefe de Google Hal Varian, “El trabajo más sexy en los próximos 10 años será ser estadístico”, palabras sobre las que reflexionó Thomas H Davenport para publicar en el 2012 su artículo: Data Scientist: The Sexiest Job of the 21st Century donde describe el perfil que debe tener el científico de datos como el híbrido de un hacker de datos, un analista, un comunicador, y un consejero confiable, combinación extremadamente poderosa y poco común, Davenport, también señala que el científico de datos no se siente cómodo como se dice coloquialmente “con la correa corta”, es decir, debe tener la libertad de experimentar y explorar posibilidades.', 'El informe que publicó “McKinsey” en el 2011, estimó que para el mundo de grandes datos en el que vivimos, espera que la demanda por talento experto en ciencia de datos podría alcanzar de los 440 000 a 490 000 puestos de trabajo para el 2018.', 'Entre los retos tecnológicos en Ciencia de Datos a los que nos enfrentamos destacamos:\\n\\n- El volumen de datos: la genómica, la monitorización (UCI, dispositivos móviles), la ubicuidad, datos sociales, se requerirán, por una parte, nuevos métodos para el almacenamiento de datos; por otra parte, estos datos requieren nuevas aplicaciones para su integración, consulta y análisis\\n- Almacenamiento físico de los datos: los datos requieren de nuevos medios y arquitecturas para su almacenamiento y tratamiento de forma eficiente\\n- Problemas de interoperabilidad: diversos hospitales tienen diferentes sistemas de almacenamiento, tiene que haber una capa de interoperabilidad para construir sobre las soluciones de tecnologías de la información\\n- Limpieza de datos, integración, análisis, herramientas: cuando se tenga acceso a información de todo tipo: los registros de salud, información de contexto, la genómica, y el resto de datos, serán necesarias nuevas herramientas y servicios para diferenciar el ruido de los datos valiosos\\n- Interpretabilidad de los modelos obtenidos con técnicas de inteligencia artificial, impacto de los cambios en los protocolos de registro de datos y en la normativa sobre los datos registrados.', 'ElProfeAlejo es un consultor especializado en Ciencia de Datos con una amplia experiencia en diversos ámbitos, incluyendo Business Intelligence, Big Data, Data Science, Cloud Computing y Marketing Digital.', 'ElProfeAlejo nació en Lima, Perú, actualmente trabaja en São Paulo, Brasil.', 'Con más de 20 años de experiencia en varios países, ElProfeAlejo ha trabajado en proyectos internacionales relacionados con Business Intelligence, Big Data e Innovación Tecnológica, ha interactuado diariamente con consultores, gestores y directores de diversas áreas de Telefónica Brazil y Telefónica Global, lo que le ha permitido adquirir un amplio conocimiento y experiencia en el campo.', 'ElProfeAlejo posee un conocimiento sólido de diversos lenguajes, herramientas y softwares utilizados en Ciencia de Datos, como SAS, Teradata, SQL Server, R, Python, Spark, Hive, Machine Learning, Tableau, MicroStrategy, Power BI, PowerPivot, entre otros, su capacidad analítica es predominante y se destaca por su excelente colaboración en el trabajo en equipo.', 'Además, ElProfeAlejo cuenta con excelentes habilidades de comunicación en varios idiomas, conoce y sabe hablar inglés, español, portugués y francés, lo que le permite interactuar eficazmente en entornos multiculturales.', 'ElProfeAlejo trabajó como Analista de Business Intelligence en la empresa Movistar Perú, responsable por la selección de clientes objetivo para campañas de marketing, clasificación y entrega de bases de datos de clientes a terceros, y creación de presentaciones de resultados para los propietarios de productos, durante su tiempo en Movistar, participó en proyectos como Next Best Offer, Campaign Delivery Robot, Marketing Campaign Website y Datamart Customer 360°.', 'Posteriormente, ElProfeAlejo trabajó como Analista de Business Intelligence en la empresa Vivo Brasil, responsable de la selección de clientes objetivo para campañas de marketing, clasificación y entrega de bases de datos de clientes a terceros, y creación de paneles de control para las unidades de gestión, durante su tiempo en Vivo, participó en proyectos como Customer Availability Simulator y Best Campaign Offer Simulator.', 'Actualmente, ElProfeAlejo trabaja como Especialista Global Big Data en la empresa Telefónica España, forma parte del equipo de Global Processing Environment y trabaja en la extracción de datos de clientes, cálculo de KPI, análisis, modelado y preparación de informes y paneles de control según las necesidades del equipo Global de BI, además, está involucrado en proyectos como AURA La Inteligencia Artificial de Telefónica y URM Project Universal Reference Model.', 'ElProfeAlejo también ha trabajado como profesor de Data Science en la escuela Alura Latam, una plataforma en línea para estudiantes de toda América Latina, ha creado cursos relacionados con Data Science y Machine Learning, centrándose en técnicas y algoritmos de Machine Learning para resolver problemas del mundo real.', 'ElProfeAlejo ha estudiado y recibido una formación sólida a lo largo de su carrera, incluyendo una licenciatura en Ingeniería en Ciencias de la Computación de la Universidad Nacional Mayor de San Marcos en Perú y un MBA en Big Data de la FIAP en Brasil, además, ha participado en cursos y entrenamientos adicionales en áreas como Marketing BTL, Liderazgo y Gestión, y Data Science.', 'En resumen, ElProfeAlejo es un especialista altamente experimentado en Ciencia de Datos con una amplia experiencia en diversos ámbitos, posee un sólido conocimiento de herramientas y tecnologías relevantes, así como habilidades analíticas y de comunicación destacadas, su trayectoria laboral internacional en importantes empresas de telecomunicaciones demuestra su capacidad para abordar proyectos complejos y trabajar en entornos multinacionales.', 'Puedes encontrar a ElProfeAlejo en sus redes sociales favoritas como Youtube, Facebook y Linkedin, pregúntame por alguna de ellas que con gusto te las indico.', 'Youtube: Puedes encontrar a ElProfeAlejo en su canal de Youtube en el siguiente enlace: https://www.youtube.com/@ElProfeAlejo, en su canal, comparte contenido relacionado con Ciencia de Datos, Machine Learning y otros temas afines, puedes suscribirte a su canal para recibir actualizaciones sobre nuevos videos y contenido.', '¿Cuál es su Facebook?, ElProfeAlejo también tiene una página de Facebook donde comparte contenido relacionado con su trabajo y conocimientos en Ciencia de Datos, puedes visitar su página en el siguiente enlace: https://www.facebook.com/elprofealejos, si te gusta su página, puedes seguir sus publicaciones y mantenerte al tanto de las novedades.', 'Linkedin: Si deseas conectarte profesionalmente con ElProfeAlejo, puedes encontrarlo en LinkedIn a través de su perfil en el siguiente enlace: https://www.linkedin.com/in/ElProfeAlejo/, en su perfil, podrás obtener más información sobre su experiencia laboral, habilidades y logros en el campo de la Ciencia de Datos, además, puedes enviarle una solicitud de conexión para establecer contacto y seguir su actividad profesional.', 'Correo electrónico: Si deseas contactar directamente a ElProfeAlejo, puedes enviar un email a la siguiente dirección: elprofealejo.info@gmail.com, puedes utilizar este medio para realizar consultas, solicitar información adicional o establecer comunicación directa sobre posibles colaboraciones o proyectos.', 'Estos canales de contacto te permiten interactuar con ElProfeAlejo y aprovechar su experiencia y conocimientos en el campo de la Ciencia de Datos, a través de sus videos en Youtube, publicaciones en Facebook, conexiones en LinkedIn o por correo electrónico, podrás obtener información valiosa y establecer contacto directo con él.', 'ElProfeAlejo vende y ofrece una amplia gama de servicios y productos especializados en capacitaciones, entrenamientos y consultoría relacionada con Ciencia de Datos, Machine Learning e Inteligencia Artificial, para más información sobre los cursos, duración, valores y fechas de inicio de los grupos de estudio pregúntame sobre cualquiera de estos cursos: \\n\\n*Ciencia de Datos: \\n- Fundamentos de Ciencia de Datos\\n- Análisis Exploratorio de Datos\\n- Aprendizaje Automático en Ciencia de Datos\\n\\n*Machine Learning: \\n- Fundamentos de Machine Learning\\n- Aprendizaje Supervisado y No Supervisado\\n- Redes Neuronales y Deep Learning\\n\\n*Inteligencia Artificial: \\n- Introducción a la Inteligencia Artificial\\n- Aplicaciones Prácticas de la Inteligencia Artificial\\n- Ética y Responsabilidad en la Inteligencia Artificial.', 'Todos los cursos, entrenamientos y capacitaciones tienen un proyecto práctico y la finalización del curso está sujeto a nota aprobatoria.', 'Los alumnos aprobados en los cursos tienen un certificado digital y carta de recomendación laboral a nombre de la escuela ElProfeAlejo, líder en capacitaciones, reconocida en toda Latinoamérica.', 'Todos los cursos, entrenamientos y capacitaciones comienzan o inician en fechas diferentes y los valores de los cursos pueden estar sujetos a cambios, te recomiendo preguntar por un curso específico o contactar directamente a ElProfeAlejo a través de sus canales de contacto para obtener información actualizada y realizar cualquier consulta adicional.', 'Si deseas participar o inscribirte en algún curso de las áreas de Ciencia de Datos, Machine Learning o Inteligencia Artificial, puedes enviarnos un correo electrónico a la siguiente dirección: elprofealejo.info@gmail.com con el curso seleccionado y entraremos en contacto para indicarte los pasos a seguir.', 'Si deseas subscribir o registrarte en algún curso de las áreas de Ciencia de Datos, Machine Learning o Inteligencia Artificial, puedes enviarnos un correo electrónico a la siguiente dirección: elprofealejo.info@gmail.com con el curso seleccionado y entraremos en contacto para indicarte los pasos a seguir.', \"El curso 'Fundamentos de Ciencia de Datos' tiene un tiempo de duración de 8 semanas, por un precio o costo de $500 USD y con fecha de inicio en 1 de julio de 2023.\", \"El curso 'Análisis Exploratorio de Datos' tiene tiempo de duración de 6 semanas, por un precio o costo de $400 USD y con fecha de inicio en 15 de agosto de 2023.\", \"El curso 'Aprendizaje Automático en Ciencia de Datos' tiene tiempo de duración de 10 semanas, por un precio o costo de $700 USD y con fecha de inicio en 5 de septiembre de 2023.\", \"El curso 'Fundamentos de Machine Learning' tiene tiempo de duración de 8 semanas, por un precio o costo de $500 USD y con fecha de inicio en 1 de julio de 2023.\", \"El curso 'Aprendizaje Supervisado y No Supervisado' tiene tiempo de duración de 6 semanas, por un precio o costo de $400 USD y con fecha de inicio en 15 de agosto de 2023.\", \"El curso 'Redes Neuronales y Deep Learning' tiene tiempo de duración de 10 semanas, por un precio o costo de $700 USD y con fecha de inicio en 5 de septiembre de 2023.\", \"El curso 'Introducción a la Inteligencia Artificial' tiene tiempo de duración de 8 semanas, por un precio o costo de $500 USD y con fecha de inicio en 1 de julio de 2023.\", \"El curso 'Aplicaciones Prácticas de la Inteligencia Artificial' tiene tiempo de duración de 6 semanas, por un precio o costo de $400 USD y con fecha de inicio en 15 de agosto de 2023.\", \"El curso 'Ética y Responsabilidad en la Inteligencia Artificial' tiene tiempo de duración de 4 semanas, por un precio o costo de $300 USD y con fecha de inicio en 5 de septiembre de 2023.\"]\n",
            "[\"['datascience', 'ciencia', 'dato', 'ser', 'campo', 'academico', 'interdisciplinario', ('utilizar', 1.0), 'estadistico', 'computacion', 'cientifico', 'metodo', 'proceso', 'algoritmo', 'sistema', ('obtener', 1.0), ('recolectar', 0.8523809523809524), ('extraer', 1.0), ('tratar', 1.0), ('analizar', 1.0), ('presentar', 1.0), 'informe', 'partir', 'ruidoso', 'estructurado', 'no']\", \"['datascience', 'ciencia', 'dato', 'ser', 'multifacetico', 'poder', ('describir el', 0.9166666666666666), 'paradigma', 'investigacion', 'metodo', 'disciplina', 'flujo', 'trabajo', 'profesion']\", \"['datascience', 'ciencia', 'dato', ('integrar', 1.0), 'conocimiento', 'dominio', 'aplicacion', 'subyacente', 'ejemplo', 'economico', 'finanza', 'medicina', 'natural', 'tecnologia', 'informacion', 'ser', 'concepto', 'consistente', ('unificar', 1.0), 'estadistico', 'analisis', 'informatica', 'metodo', 'relacionado', ('comprender', 1.0), ('analizar', 1.0), 'fenomeno', 'real']\", \"['datascience', 'ciencia', 'dato', ('utilizar', 1.0), 'metodo', 'tecnica', 'teoria', 'extraido', 'campo', 'dentro', 'contexto', 'matematica', 'estadistica', 'computacion', 'informacion', 'conocimiento', 'dominio']\", \"['datascience', 'ciencia', 'dato', 'ser', 'diferente', 'informatica', 'estadistica', 'informacion', 'ganador', 'premio', 'turing', 'jim', 'gray', ('imaginar', 1.0), 'cuarto', 'paradigma', 'empirico', 'teorico', 'computacional', 'ahora', 'basado', ('afirmar', 1.0), 'todo', 'estar', ('cambiar', 1.0), 'debido', 'impacto', 'tecnologia', 'avalancha']\", \"['datascientist', 'cientifico', 'dato', 'ser', 'profesional', 'escritura', 'aplicacion', 'codigo', 'programacion', 'conocimiento', 'estadistico', ('trabajar', 1.0), 'recoleccion', 'limpieza', 'exploracion', 'modelacion', 'visualizacion', 'implementacion', 'solucion', 'aprendizaje', 'automatico', 'interpretacion', 'resultado']\", \"['datascientist', 'perfil', 'cientifico', 'dato', ('provenir', 0.8690476190476191), 'profesion', 'backgrounds', 'matematico', 'ingeniero', 'economistas', 'actuario', 'fisico', 'quimico', 'ocasion', 'campo', 'poder', ('parecer', 0.9047619047619048), 'mucho', 'distante', 'medicina', 'sociologia']\", \"['datascience', '1962', 'john', 'w', 'tukey', ('mencionar', 1.0), 'termino', 'ciencia', 'datos', 'articulo', 'the', 'future', 'of', 'data', 'analysis', ('explicar', 1.0), 'evolucion', 'estadistica', 'matematico', 'este', ('definir', 1.0), 'primero', 'vez', 'analisis', 'dato', 'procedimientos', ('analizar', 1.0), 'tecnica', ('interpretar', 1.0), 'resultado', 'dicho', 'procedimiento', 'forma', ('planificar', 1.0), 'recopilacion', ('hacer', 1.0), 'mas', 'facil', 'preciso', 'acertado', 'maquinaria', 'el', ('aplicar', 1.0)]\", \"['1977', 'john', 'w', 'tukey', ('publicar', 1.0), 'exploratory', 'data', 'analysis', ('argumentar', 1.0), 'ser', 'necesario', ('poner', 1.0), 'mas', 'enfasis', 'uso', 'dato', ('sugerir', 1.0), 'hipotesis', ('probar', 1.0), 'modelo', 'estadistico']\", \"['datascience', 'ciencia', 'dato', 'haber', ('resultar', 1.0), 'mucho', 'disciplina', 'reciente', 'creacion', 'realidad', 'concepto', 'el', ('utilizar', 1.0), 'primero', 'vez', 'cientifico', 'danes', 'peter', 'naur', 'decada', 'sesenta', 'sustituto', 'computacional']\", \"['datascience', '1974', 'peter', 'naur', ('publicar', 1.0), 'libro', 'concise', 'survey', 'of', 'computer', 'methods', 'donde', ('utilizar', 1.0), 'ampliamente', 'concepto', 'ciencia', 'dato', 'el', ('permitir', 1.0), 'utilizacion', 'mas', 'libre', 'mundo', 'academico']\", \"['1977', 'international', 'association', 'for', 'statistical', 'computing', 'iasc', 'ser', ('establecer', 1.0), 'seccion', 'institute', 'isi', 'mision', ('relacionar', 1.0), 'metodologia', 'estadistico', 'tradicional', 'tecnologia', 'computacional', 'moderno', 'conocimiento', 'experto', 'tema', ('convertir', 1.0), 'dato', 'informacion']\", \"['datascience', '1996', 'termino', 'ciencia', 'datos', 'ser', ('utilizar', 1.0), 'primero', 'vez', 'conferencia', 'llamado', 'dato', 'clasificacion', 'metodo', 'relacionado', ('tener', 1.0), 'lugar', 'reunion', 'miembro', 'international', 'federation', 'of', 'classification', 'societies', 'ifcs', 'sede', 'kobe', 'japon']\", \"['datascience', '1997', 'jeff', 'wu', ('describir', 1.0), 'trabajo', 'estadistico', 'trilogia', 'conformado', 'recoleccion', 'dato', 'analisis', 'modelado', 'toma', 'decision', ('hacer', 1.0), 'peticion', 'estadistica', 'ser', ('renombrar', 0.9259259259259259), 'ciencia', 'cientifico']\", \"['datascience', '2001', 'william', 's', 'cleveland', ('introducir', 1.0), 'ciencia', 'dato', 'disciplina', 'independiente', ('extender', 1.0), 'campo', 'estadistica', ('incluir', 1.0), 'avance', 'computacion', 'articulo', 'data', 'science', 'an', 'action', 'plan', 'for', 'expanding', 'the', 'technical', 'area', 'of', 'field', 'statistics', ('establecer', 1.0), 'seis', 'tecnico', 'opinion', ('conformar', 0.9259259259259259), 'investigacion', 'multidisciplinaria', 'modelo', 'metodo', 'pedagogia', 'evaluacion', 'herramienta', 'teoria']\", \"['datascience', 'abril', '2002', 'international', 'council', 'for', 'science', 'committee', 'on', 'data', 'and', 'technology', 'codata', ('empezar', 1.0), 'publicacion', 'journal', 'enfocado', 'problema', 'descripcion', 'sistema', 'dato', 'internet', 'aplicacion', 'legal', 'poco', 'despues', 'enero', '2003', 'universidad', 'columbia', ('publicar', 1.0), 'the', 'of', ('ofrecer', 1.0), 'plataforma', 'profesional', ('presentar', 1.0), 'perspectiva', ('intercambiar', 1.0), 'idea']\", \"['2005', 'the', 'national', 'science', 'board', ('publicar', 1.0), 'long-lived', 'digital', 'data', 'collections', 'enabling', 'research', 'and', 'education', 'century', ('definir', 1.0), 'cientifico', 'dato', 'computacion', 'informacion', 'programador', 'base', 'software', 'experto', 'disciplinario', 'ser', 'crucial', 'gestion', 'exitoso', 'coleccion', 'cuyo', 'actividad', 'primario', ('realizar', 1.0), 'investigacion', 'creativo', 'analisis', '2008', 'jeff', 'hammerbacher', 'dj', 'patil', 'el', ('utilizar', 0.9333333333333332), 'trabajo', 'realizado', 'facebook', 'linkedin', 'respectivamente']\", \"['datascience', '2009', 'investigador', 'yangyong', 'zhu', 'yun', 'xiong', 'research', 'center', 'for', 'dataology', 'and', 'data', 'science', ('publicar', 1.0), 'introduction', 'to', 'donde', ('manifestar', 1.0), 'diferencia', 'ciencia', 'natural', 'social', 'datologia', 'datos', ('tomar', 1.0), 'dato', 'red', 'objeto', 'estudio', '2013', 'ser', ('lanzar', 1.0), 'ieee', 'task', 'force', 'on', 'advanced', 'analytics', 'primero', 'conferencia', 'internacional', 'international', 'conference', '2014']\", \"['datascience', '2015', 'international', 'journal', 'on', 'data', 'science', 'and', 'analytics', 'ser', ('lanzar', 1.0), 'springer', ('publicar', 1.0), 'trabajo', 'original', 'ciencia', 'dato', 'analitico', 'big', ('datar', 1.0)]\", \"['datascientist', 'datascience', 'roadmap', ('convertir el', 0.9166666666666666), 'cientifico', 'dato', ('existir', 1.0), 'forma', ('adquirir', 1.0), 'conocimiento', 'necesario', 'universidad', 'estar', ('empezar', 1.0), ('ofrecer', 1.0), 'curso', 'diplomado', 'alguno', 'maestria', 'doctorado', 'ciencia']\", \"['datascience', 'ibm', 'network', 'skills', ('ofrecer', 1.0), 'certificado', 'profesional', 'ciencia', 'datos', ('tener', 1.0), 'costar', 'el', 'poder', ('pedir', 1.0), 'ayuda', 'economico', ('cursar el', 0.8518518518518517), 'gratuitamente', 'estar', 'compuesto', '9', 'curso', 'duracion', '10', 'mes', 'mas', 'informacion', 'https//wwwcourseraorg/professional-certificates/certificado-profesional-de-ciencia-de-datos-de-ibm']\", \"['datascience', 'universidad', 'peruana', 'ciencias', 'aplicado', 'traves', 'escuela', 'postgrado', 'tambien', ('ofrecer', 1.0), 'cursos', 'ciencia', 'datos', 'ser', 'curso', 'corto', 'modalidad', 'online', 'virtual', 'distribuido', 'categoria', 'flex', 'courses', '6', 'hora', 'academico', 'especializados', '24']\", \"['datascience', 'ejemplo', 'aplicacion', 'ciencia', 'datos', 'marketing', 'septiembre', '1994', 'businessweek', ('publicar', 1.0), 'articulo', 'base', 'dato', ('manifestar', 1.0), 'empresa', ('recopilar', 1.0), 'gran', 'cantidad', 'informacion', 'cliente', 'ser', ('analizar', 1.0), ('predecir', 1.0), 'probabilidad', ('comprar', 1.0), 'producto', ('afirmar', 1.0), 'el', ('utilizar', 1.0), 'conocimiento', ('elaborar', 1.0), 'mensaje', 'calibrado', 'precision', 'individuo', ('buscar', 1.0), ('conseguir el', 0.9166666666666666), 'asimismo', ('explicar', 1.0), 'ochenta', 'entusiasmo', 'provocado', 'propagacion', 'lector', 'codigo', 'barra', ('terminar', 1.0), 'decepcion', 'generalizado', 'pues', ('abrumar', 0.9047619047619048), ('lograr', 1.0), ('hacer', 1.0), 'algo', 'util', 'embargo', ('creer', 1.0), 'no', 'haber', 'mas', 'remedio', ('desafinar', 0.9629629629629629), 'frontera', ('desarrollar', 1.0), 'tecnologia', 'necesario']\", \"['datascience', 'ejemplo', 'aplicacion', 'ciencia', 'datos', 'marketing', '2014', 'empresa', 'sueco', 'musica', 'streaming', 'spotify', ('comprar', 1.0), 'the', 'echo', 'nest', 'compañia', 'especializado', 'dato', 'musical', 'este', 'ahora', 'ser', 'encargada', ('almacenar', 1.0), ('analizar', 1.0), 'informacion', 'su', '170', 'millon', 'usuario', 'ayuda', 'dicho', '2015', ('lanzar', 1.0), 'servicio', 'personalizado', 'llamado', 'discover', 'weekly', 'semanalmente', ('recomendar', 1.0), 'seleccion', 'cancion', 'poder', ('interesar el', 0.9166666666666666), 'medio', 'algoritmo', 'analisis', 'escuchado', 'historial', 'busqueda', 'semana', 'pasado', ('recibir', 1.0), 'buen', 'recepcion', 'generalizado', 'actualmente', ('figurar', 1.0), 'fuerte', 'punto', 'venta', 'competencia']\", \"['datascience', 'ejemplo', 'aplicacion', 'ciencia', 'datos', 'marketing', 'netflix', 'empresa', 'norteamericano', 'contenido', 'multimedia', 'streaming', ('ofrecer', 1.0), 'mas', '120', 'millon', 'usuario', 'plataforma', 'capaz', ('analizar', 1.0), 'algoritmo', 'costumbre', 'consumo', ('diferenciar', 1.0), 'este', ('buscar', 1.0), ('lograr', 1.0), ('determinar', 1.0), 'nuevo', 'el', 'poder', ('interesar', 1.0), 'todd', 'yellin', 'vicepresidente', 'producto', ('explicar', 1.0), 'alguno', 'dato', 'almacenado', ('extender el', 0.9090909090909092), 'hora', 'dia', ('conectar', 1.0), 'tiempo', ('pasar', 1.0), 'dentro', 'lista', 'recientemente', 'visto', 'incluso', 'orden', 'especifico', 'informacion', ('almacenar', 1.0), 'ser', ('utilizar', 1.0), 'especificamente', ('aprender', 1.0), ('dar el', 0.8333333333333334), 'recomendacion', 'acertado']\", \"['datascience', 'ejemplo', 'aplicacion', 'ciencia', 'datos', 'gobernanza', 'america', 'latina', 'banco', 'interamericano', 'desarrollo', 'bid', 'haber', ('desarrollar', 1.0), 'estudio', 'exploratorio', 'el', ('analizar', 1.0), 'dato', 'implementacion', 'diseño', 'politica', 'publico', 'region', ('tomar', 1.0), 'caso', 'pais', 'argentina', 'brasil', ('presentar', 1.0), 'recomendacion', 'mantenimiento', 'este', ('ir', 1.0), 'tema', 'movilidad', 'urbano', 'sostenible', 'ciudad', 'inteligente', 'seguridad', 'propiedad', 'privacidad', 'sugerencia', 'presentado', 'investigacion', ('estar', 1.0), ('lograr', 1.0), 'inteligencia', 'valor', ('tener', 1.0), 'potencialidad', 'ser', 'componente', 'estrategico', 'toma', 'decision', 'evaluacion', 'otro', 'capacidad', 'campo', 'mejora', 'rendicion', 'contar', 'gobierno', 'ciudadania', ('promover', 0.8690476190476191), 'avance', 'curaduria', 'institucion']\", \"['textualmente', 'big', 'data', 'el', ('referir', 1.0), 'enorme', 'volumen', 'dato', 'no', 'poder', ('procesar el', 0.9090909090909092), 'manera', 'efectivo', 'aplicacion', 'tradicional', 'actualmente', ('aplicar', 1.0), 'acuerdo', 'guia', 'amazon', 'web', 'service', ('considerar', 1.0), 'coleccion', 'considerable', 'dificultad', ('almacenar el', 0.9166666666666666), 'base', 'tambien', 'servidor', 'estandar', ('analizar el', 0.9090909090909092), 'habitual']\", \"['datascience', 'big', 'data', 'el', ('soler', 1.0), ('relacionar', 1.0), 'ciencia', 'dato', 'pues', 'ese', 'ser', 'fuente', 'informacion', 'analisis']\", \"['datascience', 'ciencia', 'dato', ('lograr', 1.0), ('trabajar', 1.0), ('analizar', 1.0), 'grande', 'conjunto', 'desordenado', 'incompleto', ('llegar', 1.0), 'hallazgo', ('impulsar', 1.0), 'decision', 'operacion', 'producto']\", \"['datascientist', 'el', ('definir', 1.0), 'cientifico', 'dato', 'mezcla', 'estadistico', 'computologo', 'pensador', 'creativo', 'siguiente', 'habilidad', ('recopilar', 1.0), ('procesar', 1.0), ('extraer', 1.0), 'valor', 'diverso', 'extenso', 'base', 'imaginacion', ('comprender', 1.0), ('visualizar', 1.0), ('comunicar', 1.0), 'conclusion', 'no', 'capacidad', ('crear', 1.0), 'solucion', 'basado', ('aumentar', 1.0), 'beneficio', ('reducir', 1.0), 'costo']\", \"['datascientist', 'proceso', ('seguir', 1.0), 'cientifico', 'dato', ('trabajar', 1.0), ('resolver', 1.0), 'problema', 'el', 'poder', ('resumir', 1.0), 'paso', ('extraer', 1.0), 'independientemente', 'fuente', 'volumen', ('limpiar', 1.0), ('eliminar', 1.0), ('sesgar', 0.8222222222222223), 'resultado', ('procesar', 1.0), ('usar', 1.0), 'metodo', 'estadistico', 'inferencia', 'modelo', 'regresion', 'prueba', 'hipotesis', ('diseñar', 1.0), 'experimento', 'adicional', 'caso', 'ser', 'necesario', ('crear', 1.0), 'visualizacion', 'grafico', 'relevante', 'investigacion']\", \"['datascientist', 'cientifico', 'dato', 'ser', 'estadistico', 'deber', ('saber', 1.0), ('aprender', 1.0), 'interfaz', 'programacion', 'aplicacion', 'api', 'base', 'extraccion', 'diseñador', ('programar', 1.0), 'computologo', 'saber', ('analizar', 1.0), ('encontrar', 1.0), 'significado']\", \"['tesis', 'doctoral', 'benjamin', 'fry', ('explicar', 1.0), 'proceso', ('comprender', 1.0), 'mejor', 'dato', ('comenzar', 1.0), 'serie', 'numero', 'objetivo', ('responder', 1.0), 'pregunta', 'fase', 'el', ('proponer', 1.0), ('adquirir', 1.0), ('analizar', 1.0), ('filtrar', 1.0), ('extraer', 1.0), ('representar', 1.0), ('refinar', 1.0), ('interactuar', 0.837121212121212), ('requerir', 1.0), 'enfoque', 'especializado', ('aportir', 0.9047619047619048), 'comprension', ('mencionar', 1.0), 'estar', 'ingeniero', 'sistema', 'matematico', 'estadistico', 'diseñador', 'grafico', 'especialista', 'visualizacion', 'informacion', 'interaccion', 'hombre-maquin', 'conocido', 'sigla', 'ingles', 'hci', 'human-computer', 'interaction', 'ademas', ('afirmar', 1.0), ('contar', 1.0), 'lejos', ('resolver', 1.0), 'problema', 'entendimiento', ('convertir', 1.0), 'parte', 'ya', 'especializacion', ('conducir', 1.0), 'manera', 'aislado', 'camino', 'solucion', 'poder', ('perder', 1.0), 'algo', 'transicion']\", \"['datascience', 'drew', 'conway', 'pagina', 'web', ('explicar', 1.0), 'ayuda', 'diagrama', 'venn', 'principal', 'habilidad', 'el', ('dar', 1.0), 'vida', 'forma', 'ciencia', 'dato', 'asi', 'relacion', 'conjunto']\", \"['datascientist', 'datascience', 'ciencia', 'dato', 'haber', ('cobrar', 1.0), 'recientemente', 'importancia', ('acontecer', 0.8518518518518517), 'disciplina', 'profesion', 'emergente', 'cientifico', 'el', ('volver', 1.0), 'foco', 'atencion', 'vez', 'mas', 'organizacion', 'nivel', 'mundial', 'tal', ('señalar', 1.0), 'economista', 'jefe', 'google', 'hal', 'varian', 'trabajo', 'sexy', 'proximo', '10', 'año', 'ser', 'estadistico', 'palabra', ('reflexionar', 1.0), 'thomas', 'h', 'davenport', ('publicar', 1.0), '2012', 'articulo', 'data', 'scientist', 'the', 'sexiest', 'job', 'of', 'century', 'donde', ('describir', 1.0), 'perfil', 'deber', ('tener', 1.0), 'hibrido', 'hacker', 'analista', 'comunicador', 'consejero', 'confiable', 'combinacion', 'extremadamente', 'poderoso', 'poco', 'comun', 'tambien', 'no', ('sentir', 1.0), 'comodo', ('decir', 1.0), 'coloquialmente', 'correa', 'corto', 'decir', 'libertad', ('experimentar', 1.0), ('explorar', 1.0), 'posibilidad']\", \"['datascience', 'informe', ('publicar', 1.0), 'mckinsey', '2011', ('estimar', 1.0), 'mundo', 'grande', 'dato', ('vivir', 1.0), ('esperar', 1.0), 'demanda', 'talento', 'experto', 'ciencia', 'poder', ('alcanzar', 0.8690476190476191), '440', '000', '490', 'puesto', 'trabajo', '2018']\", \"['datascience', 'reto', 'tecnologico', 'ciencia', 'datos', 'yo', ('enfrentar', 0.8842592592592592), ('destacar', 1.0), 'volumen', 'dato', 'genomica', 'monitorizacion', 'uci', 'dispositivo', 'movil', 'ubicuidad', 'social', 'el', ('requerir', 1.0), 'parte', 'nuevo', 'metodo', 'almacenamiento', 'aplicacion', 'integracion', 'consulta', 'analisis', 'fisico', 'medio', 'arquitectura', 'tratamiento', 'forma', 'eficiente', 'problema', 'interoperabilidad', 'hospital', ('tener', 1.0), 'sistema', 'haber', 'capa', ('construir', 1.0), 'solucion', 'tecnologia', 'informacion', 'limpieza', 'herramienta', 'acceso', 'tipo', 'registro', 'salud', 'contexto', 'resto', 'ser', 'necesario', 'servicio', ('diferenciar', 1.0), 'ruido', 'valioso', 'interpretabilidad', 'modelo', 'obtenido', 'tecnica', 'inteligencia', 'artificial', 'impacto', 'cambio', 'protocolo', 'normativa', 'registrado']\", \"['elprofealejo', 'datascience', 'ser', 'consultor', 'especializado', 'ciencia', 'datos', 'amplio', 'experiencia', 'ambito', ('incluir', 1.0), 'business', 'intelligence', 'big', 'data', 'science', 'cloud', 'computing', 'marketing', 'digital']\", \"['elprofealejo', ('nacer', 1.0), 'lima', 'peru', 'actualmente', ('trabajar', 1.0), 'são', 'paulo', 'brasil']\", \"['elprofealejo', 'mas', '20', 'año', 'experiencia', 'pais', 'haber', ('trabajar', 1.0), 'proyecto', 'internacional', 'relacionado', 'business', 'intelligence', 'big', 'data', 'innovacion', 'tecnologica', ('interactuar', 0.837121212121212), 'diariamente', 'consultor', 'gestor', 'director', 'area', 'telefonica', 'brazil', 'global', 'el', ('permitir', 1.0), ('adquirir', 1.0), 'amplio', 'conocimiento', 'campo']\", \"['elprofealejo', 'datascience', ('poseer', 1.0), 'conocimiento', 'solido', 'lenguaje', 'herramienta', 'softwares', 'utilizado', 'ciencia', 'datos', 'sas', 'teradata', 'sql', 'server', 'r', 'python', 'spark', 'hive', 'machine', 'learning', 'tableau', 'microstrategy', 'power', 'bi', 'powerpivot', 'otro', 'capacidad', 'analitico', 'ser', 'predominante', 'el', ('destacar', 1.0), 'excelente', 'colaboracion', 'trabajo', 'equipo']\", \"['elprofealejo', 'ademas', ('contar', 1.0), 'excelente', 'habilidad', 'comunicacion', 'idioma', ('conocer', 1.0), ('saber', 1.0), ('hablar', 1.0), 'ingles', 'español', 'portugues', 'frances', 'el', ('permitir', 1.0), ('interactuar', 0.837121212121212), 'eficazmente', 'entorno', 'multicultural']\", \"['elprofealejo', ('trabajar', 1.0), 'analista', 'business', 'intelligence', 'empresa', 'movistar', 'peru', 'responsable', 'seleccion', 'cliente', 'objetivo', 'campaña', 'marketing', 'clasificacion', 'entrega', 'base', 'dato', 'tercero', 'creacion', 'presentacion', 'resultado', 'propietario', 'producto', 'tiempo', ('participar', 1.0), 'proyecto', 'next', 'best', 'offer', 'campaign', 'delivery', 'robot', 'website', 'datamart', 'customer', '360']\", \"['elprofealejo', 'posteriormente', ('trabajar', 1.0), 'analista', 'business', 'intelligence', 'empresa', 'vivo', 'brasil', 'responsable', 'seleccion', 'cliente', 'objetivo', 'campaña', 'marketing', 'clasificacion', 'entrega', 'base', 'dato', 'tercero', 'creacion', 'panel', 'control', 'unidad', 'gestion', 'tiempo', ('participar', 1.0), 'proyecto', 'customer', 'availability', 'simulator', 'best', 'campaign', 'offer']\", \"['elprofealejo', 'actualmente', ('trabajar', 1.0), 'especialista', 'global', 'big', 'data', 'empresa', 'telefonica', 'españa', ('formar', 1.0), 'parte', 'equipo', 'processing', 'environment', 'extraccion', 'dato', 'cliente', 'calculo', 'kpi', 'analisis', 'modelado', 'preparacion', 'informe', 'panel', 'control', 'necesidad', 'bi', 'ademas', 'estar', 'involucrado', 'proyecto', 'aura', 'inteligencia', 'artificial', 'urm', 'project', 'universal', 'reference', 'model']\", \"['elprofealejo', 'datascience', 'tambien', 'haber', ('trabajar', 1.0), 'profesor', 'data', 'science', 'escuela', 'alura', 'latam', 'plataforma', 'linea', 'estudiante', 'america', 'latina', ('crear', 1.0), 'curso', 'relacionado', 'machine', 'learning', ('centrar el', 0.9), 'tecnica', 'algoritmo', ('resolver', 1.0), 'problema', 'mundo', 'real']\", \"['elprofealejo', 'datascience', 'haber', ('estudiar', 1.0), ('recibir', 1.0), 'formacion', 'solido', 'el', 'largo', 'carrera', ('incluir', 1.0), 'licenciatura', 'ingenieria', 'ciencias', 'computacion', 'universidad', 'nacional', 'mayor', 'san', 'marcos', 'peru', 'mba', 'big', 'data', 'fiap', 'brasil', 'ademas', ('participar', 1.0), 'curso', 'entrenamiento', 'adicional', 'area', 'marketing', 'btl', 'liderazgo', 'gestion', 'science']\", \"['elprofealejo', 'datascience', 'resumen', 'ser', 'especialista', 'altamente', 'experimentado', 'ciencia', 'datos', 'amplio', 'experiencia', 'ambito', ('poseer', 1.0), 'solido', 'conocimiento', 'herramienta', 'tecnologia', 'relevante', 'asi', 'habilidad', 'analitica', 'comunicacion', 'destacado', 'trayectoria', 'laboral', 'internacional', 'importante', 'empresa', 'telecomunicacion', ('demostrar', 1.0), 'capacidad', ('abordar', 1.0), 'proyecto', 'complejo', ('trabajar', 1.0), 'entorno', 'multinacional']\", \"['elprofealejo', 'poder', ('encontrar', 1.0), 'red', 'social', 'favorito', 'youtube', 'facebook', 'linkedin', ('preguntar', 1.0), 'alguno', 'el', 'gusto', 'tu', ('indico', 0.8492063492063492)]\", \"['elprofealejo', 'datascience', 'youtube', 'poder', ('encontrar', 1.0), 'canal', 'siguiente', 'enlace', 'https//wwwyoutubecom/elprofealejo', ('compartir', 1.0), 'contenido', 'relacionado', 'ciencia', 'datos', 'machine', 'learning', 'tema', 'afin', ('suscribir', 0.9666666666666667), ('recibir', 1.0), 'actualizacion', 'nuevo', 'video']\", \"['elprofealejo', 'datascience', 'ser', 'facebook', 'tambien', ('tener', 1.0), 'pagina', 'donde', ('compartir', 1.0), 'contenido', 'relacionado', 'trabajo', 'conocimiento', 'ciencia', 'datos', 'poder', ('visitar', 1.0), 'siguiente', 'enlace', 'https//wwwfacebookcom/elprofealejo', 'si', 'tu', ('gustar', 1.0), ('seguir', 1.0), 'publicacion', ('mantener', 0.9629629629629629), 'tanto', 'novedad']\", \"['elprofealejo', 'datascience', 'linkedin', 'si', ('desear', 1.0), ('conectar', 0.9629629629629629), 'profesionalmente', 'poder', ('encontrar el', 0.9166666666666666), 'traves', 'perfil', 'siguiente', 'enlace', 'https//wwwlinkedincom/in/elprofealejo/', 'podrar', ('obtener', 1.0), 'mas', 'informacion', 'experiencia', 'laboral', 'habilidad', 'logro', 'campo', 'ciencia', 'datos', 'ademas', ('enviar el', 0.8888888888888888), 'solicitud', 'conexion', ('establecer', 1.0), 'contacto', ('seguir', 1.0), 'actividad', 'profesional']\", \"['elprofealejo', 'correo', 'electronico', 'si', ('desear', 1.0), ('contactar', 1.0), 'directamente', 'poder', ('enviar', 1.0), 'email', 'siguiente', 'direccion', ('utilizar', 1.0), 'medio', ('realizar', 1.0), 'consulta', ('solicitar', 1.0), 'informacion', 'adicional', ('establecer', 1.0), 'comunicacion', 'directo', 'posible', 'colaboracion', 'proyecto']\", \"['elprofealejo', 'datascience', 'canal', 'contacto', 'tu', ('permitir', 1.0), ('interactuar', 0.837121212121212), ('aprovechar', 1.0), 'experiencia', 'conocimiento', 'campo', 'ciencia', 'datos', 'traves', 'video', 'youtube', 'publicacion', 'facebook', 'conexion', 'linkedin', 'correo', 'electronico', 'podrar', ('obtener', 1.0), 'informacion', 'valioso', ('establecer', 1.0), 'directo', 'el']\", \"['elprofealejo', 'datascience', ('vender', 1.0), ('ofrecer', 1.0), 'amplio', 'gama', 'servicio', 'producto', 'especializado', 'capacitacion', 'entrenamiento', 'consultoria', 'relacionado', 'ciencia', 'datos', 'machine', 'learning', 'inteligencia', 'artificial', 'mas', 'informacion', 'curso', 'duracion', 'valor', 'fecha', 'inicio', 'grupo', 'estudio', ('preguntar', 0.9393939393939394), 'cualquiera', 'fundamentos', 'analisis', 'exploratorio', 'aprendizaje', 'automatico', '*', 'supervisado', 'no', 'redes', 'neuronales', 'deep', 'introduccion', 'aplicaciones', 'practicas', 'etico', 'responsabilidad']\", \"['curso', 'entrenamiento', 'capacitacion', ('tener', 1.0), 'proyecto', 'practico', 'finalizacion', 'estar', 'sujeto', 'nota', 'aprobatorio']\", \"['elprofealejo', 'alumno', 'aprobado', 'curso', ('tener', 1.0), 'certificado', 'digital', 'carta', 'recomendacion', 'laboral', 'nombre', 'escuela', 'lider', 'capacitacion', 'reconocido', 'latinoamerica']\", \"['elprofealejo', 'curso', 'entrenamiento', 'capacitacion', ('comenzar', 1.0), ('iniciar', 1.0), 'fecha', 'diferente', 'valor', 'poder', 'estar', 'sujeto', 'cambio', 'tu', ('recomendar', 1.0), ('preguntar', 1.0), 'especifico', ('contactar', 1.0), 'directamente', 'traves', 'canal', 'contacto', ('obtener', 1.0), 'informacion', 'actualizado', ('realizar', 1.0), 'consulta', 'adicional']\", \"['elprofealejo', 'datascience', 'si', ('desear', 1.0), ('participar', 1.0), ('inscribir', 0.9666666666666667), 'curso', 'area', 'ciencia', 'datos', 'machine', 'learning', 'inteligencia', 'artificial', 'poder', ('enviar yo', 0.8888888888888888), 'correo', 'electronico', 'siguiente', 'direccion', ('elprofealejoinfogmailcom', 0.6712962962962963), 'seleccionado', ('entrarer', 0.9166666666666666), 'contacto', ('indicar', 0.9583333333333334), 'paso', ('seguir', 1.0)]\", \"['elprofealejo', 'datascience', 'si', ('desear', 1.0), ('subscribir', 0.8925925925925925), ('registrar', 0.9666666666666667), 'curso', 'area', 'ciencia', 'datos', 'machine', 'learning', 'inteligencia', 'artificial', 'poder', ('enviar yo', 0.8888888888888888), 'correo', 'electronico', 'siguiente', 'direccion', ('elprofealejoinfogmailcom', 0.6712962962962963), 'seleccionado', ('entrarer', 0.9166666666666666), 'contacto', ('indicar', 0.9583333333333334), 'paso', ('seguir', 1.0)]\", \"['datascience', 'curso', 'fundamentos', 'ciencia', 'datos', ('tener', 1.0), 'tiempo', 'duracion', '8', 'semana', 'precio', 'costar', '500', 'usd', 'fecha', 'inicio', '1', 'julio', '2023']\", \"['curso', 'analisis', 'exploratorio', 'datos', ('tener', 1.0), 'tiempo', 'duracion', '6', 'semana', 'precio', 'costar', '400', 'usd', 'fecha', 'inicio', '15', 'agosto', '2023']\", \"['datascience', 'curso', 'aprendizaje', 'automatico', 'ciencia', 'datos', ('tener', 1.0), 'tiempo', 'duracion', '10', 'semana', 'precio', 'costar', '700', 'usd', 'fecha', 'inicio', '5', 'septiembre', '2023']\", \"['curso', 'fundamentos', 'machine', 'learning', ('tener', 1.0), 'tiempo', 'duracion', '8', 'semana', 'precio', 'costar', '500', 'usd', 'fecha', 'inicio', '1', 'julio', '2023']\", \"['curso', 'aprendizaje', 'supervisado', 'no', ('tener', 1.0), 'tiempo', 'duracion', '6', 'semana', 'precio', 'costar', '400', 'usd', 'fecha', 'inicio', '15', 'agosto', '2023']\", \"['curso', 'redes', 'neuronales', 'deep', 'learning', ('tener', 1.0), 'tiempo', 'duracion', '10', 'semana', 'precio', 'costar', '700', 'usd', 'fecha', 'inicio', '5', 'septiembre', '2023']\", \"['curso', 'introduccion', 'inteligencia', 'artificial', ('tener', 1.0), 'tiempo', 'duracion', '8', 'semana', 'precio', 'costar', '500', 'usd', 'fecha', 'inicio', '1', 'julio', '2023']\", \"['curso', 'aplicaciones', 'practicas', 'inteligencia', 'artificial', ('tener', 1.0), 'tiempo', 'duracion', '6', 'semana', 'precio', 'costar', '400', 'usd', 'fecha', 'inicio', '15', 'agosto', '2023']\", \"['curso', 'etica', 'responsabilidad', 'inteligencia', 'artificial', ('tener', 1.0), 'tiempo', 'duracion', '4', 'semana', 'precio', 'costar', '300', 'usd', 'fecha', 'inicio', '5', 'septiembre', '2023']\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df_dialogo.head(5)"
      ],
      "metadata": {
        "id": "1J4trlLA75JH"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Guardar DF en drive:"
      ],
      "metadata": {
        "id": "CIRV2DyvtZPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def guardar_dataframe(dataframe):\n",
        "  carpeta = \"/content/drive/MyDrive/Chatbot\"\n",
        "  nombre_archivo = \"df_dialogo_spring_02.csv\"\n",
        "\n",
        "  if not os.path.exists(carpeta):\n",
        "    os.makedirs(carpeta)\n",
        "  archivo_csv = os.path.join(carpeta, nombre_archivo)\n",
        "  dataframe.to_csv(archivo_csv, index=False)"
      ],
      "metadata": {
        "id": "w5YqjPJ2jWJD"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Buscar respuesta del Chatbot"
      ],
      "metadata": {
        "id": "IVJ6UmQV3n4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import jellyfish\n",
        "\n",
        "def dialogo(user_response):\n",
        "    # Tratamiento de texto\n",
        "    user_response = tratamiento_texto(user_response)  # Tratando el texto\n",
        "    user_response = re.sub(r\"[^\\w\\s]\", '', user_response)  # Elimina signos de puntuación\n",
        "\n",
        "    df = df_dialogo.copy()\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # Calcular la similitud de coseno usando TF-IDF\n",
        "        tfidf_matrix = tfidf_vectorizer.fit_transform([user_response, row['dialogo']])\n",
        "        cos_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
        "\n",
        "        df.at[idx, 'interseccion'] = len(set(user_response.split()) & set(row['dialogo'].split()))/len(user_response.split())\n",
        "        df.at[idx, 'similarity'] = cos_sim[0][0]\n",
        "        df.at[idx, 'jaro_winkler'] = jellyfish.jaro_winkler_similarity(user_response, row['dialogo'])\n",
        "\n",
        "        # Calcular la probabilidad:\n",
        "        df.at[idx, 'probabilidad'] = max(df.at[idx, 'interseccion'], df.at[idx, 'similarity'], df.at[idx, 'jaro_winkler'])\n",
        "\n",
        "    df.sort_values(by=['probabilidad', 'jaro_winkler'], inplace=True, ascending=False)\n",
        "\n",
        "    # Probabilidad máxima:\n",
        "    probabilidad = round(df['probabilidad'].head(1).values[0],2)\n",
        "\n",
        "    #Guardar df:\n",
        "    guardar_dataframe(df)\n",
        "\n",
        "    if probabilidad >= 0.93:\n",
        "        print('Respuesta encontrada por el método de comparación de textos - Probabilidad: ', probabilidad)\n",
        "        respuesta = df['respuesta'].head(1).values[0]\n",
        "    else:\n",
        "        respuesta = ''\n",
        "    return respuesta\n",
        "\n",
        "#Cargar tu modelo entrenado aqui(recuerda siempre cargar el modelo y el vectorizer o tokenizer usado en el entrenamiento del modelo):\n",
        "ruta_modelo = '/content/drive/MyDrive/Chatbot/modelo_transformers'\n",
        "Modelo_TF = BertForSequenceClassification.from_pretrained(ruta_modelo)\n",
        "tokenizer_TF = BertTokenizer.from_pretrained(ruta_modelo)\n",
        "\n",
        "#Función para dialogar utilizando el modelo de Machine Learning:\n",
        "def clasificacion_modelo(pregunta):\n",
        "  frase = normalizar(pregunta)\n",
        "  frase = ' '.join(str(elemento) for elemento in frase)\n",
        "  tokens = tokenizer_TF.encode_plus(\n",
        "      frase,\n",
        "      add_special_tokens=True,\n",
        "      max_length=128,\n",
        "      padding='max_length',\n",
        "      truncation=True,\n",
        "      return_tensors='pt'\n",
        "  )\n",
        "  input_ids = tokens['input_ids']\n",
        "  attention_mask = tokens['attention_mask']\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = Modelo_TF(input_ids, attention_mask)\n",
        "\n",
        "  etiquetas_predichas = torch.argmax(outputs.logits, dim=1)\n",
        "  etiquetas_decodificadas = etiquetas_predichas.tolist()\n",
        "\n",
        "  diccionario = {3: 'Continuacion', 10: 'Nombre', 2: 'Contacto', 13: 'Saludos', 14: 'Sentimiento', 9: 'Identidad', 15: 'Usuario', 6: 'ElProfeAlejo', 1: 'Aprendizaje', 0: 'Agradecimiento', 5: 'Edad', 4: 'Despedida', 11: 'Origen', 12: 'Otros', 7: 'Error', 8: 'Funcion'}\n",
        "  llave_buscada = etiquetas_decodificadas[0]\n",
        "  clase_encontrada = diccionario[llave_buscada]\n",
        "\n",
        "  #Buscar respuesta más parecida en la clase encontrada\n",
        "  df = df_dialogo[df_dialogo['tipo'] == clase_encontrada]\n",
        "  df.reset_index(inplace=True)\n",
        "  vectorizer = TfidfVectorizer()\n",
        "  dialogos_num = vectorizer.fit_transform(df['dialogo'])\n",
        "  pregunta_num = vectorizer.transform([tratamiento_texto(pregunta)])\n",
        "  similarity_scores = cosine_similarity(dialogos_num, pregunta_num)\n",
        "  indice_pregunta_proxima = similarity_scores.argmax()\n",
        "\n",
        "  if max(similarity_scores)>0.5 and clase_encontrada not in ['Otros']:\n",
        "    print('Respuesta encontrada por el modelo Transformers - tipo:',clase_encontrada)\n",
        "    respuesta = df['respuesta'][indice_pregunta_proxima]\n",
        "  else:\n",
        "    respuesta = ''\n",
        "  return respuesta\n",
        "\n",
        "# Función para devolver la respuesta de los documentos\n",
        "def respuesta_documento(pregunta):\n",
        "  pregunta = normalizar(pregunta)\n",
        "  def contar_coincidencias(frase):\n",
        "    return sum(1 for elemento in pregunta if elemento in frase)\n",
        "\n",
        "  diccionario = {valor: posicion for posicion, valor in enumerate(lista_frases_normalizadas)}\n",
        "  lista = sorted(list(diccionario.keys()), key=contar_coincidencias, reverse=True)[:6]\n",
        "\n",
        "  lista.append(''.join(pregunta))\n",
        "\n",
        "  TfidfVec = TfidfVectorizer(tokenizer=normalizar)\n",
        "  tfidf = TfidfVec.fit_transform(lista)\n",
        "\n",
        "  vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "  idx = vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = round(flat[-2],2)\n",
        "  if req_tfidf>=0.22:\n",
        "    print('Respuesta encontrada por el método TfidfVectorizer - Probabilidad:', req_tfidf)\n",
        "    respuesta = lista_frases[diccionario[lista[idx]]]\n",
        "  else:\n",
        "    respuesta = ''\n",
        "  return respuesta\n",
        "\n",
        "# Función para devolver una respuesta final buscada en todos los métodos disponibles\n",
        "def respuesta_chatbot(pregunta):\n",
        "  respuesta = dialogo(pregunta)\n",
        "  if respuesta != '':\n",
        "    return respuesta\n",
        "  else:\n",
        "    respuesta = clasificacion_modelo(pregunta)\n",
        "    if respuesta != '':\n",
        "      return respuesta\n",
        "    else:\n",
        "      preg=pregunta\n",
        "      respuesta = respuesta_documento(preg)\n",
        "      if respuesta != '':\n",
        "        return respuesta\n",
        "      else:\n",
        "        return 'Respuesta no encontrada'"
      ],
      "metadata": {
        "id": "Bpxajyck3pav"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_new=pd.read_csv('/content/drive/MyDrive/Chatbot/df_dialogo_spring_02.csv')\n",
        "# df_new.sample(5)"
      ],
      "metadata": {
        "id": "qJVB-zjXUw9T"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "2J8SlRavDohi"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Ejecutar Chatbot"
      ],
      "metadata": {
        "id": "LMEwexpz4gdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pregunta=\"ciencia de datos profe alejo?\"\n",
        "print('tokens pregunta: ',normalizar(pregunta))\n",
        "print(\"Respuesta:\")\n",
        "respuesta_documento(pregunta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "r69T28VeLA3l",
        "outputId": "94e330b6-f3c7-4ea7-81a7-6594f89f7cbf"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens pregunta:  ['elprofealejo', 'datascience', 'ciencia', 'dato', 'profe', 'alejo']\n",
            "Respuesta:\n",
            "Respuesta encontrada por el método TfidfVectorizer - Probabilidad: 0.94\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'La ciencia de datos integra el conocimiento del dominio de la aplicación subyacente (por ejemplo, ciencia económica, finanzas, medicina, ciencias naturales, tecnologías de la información) y es un \"concepto consistente en unificar estadística, análisis de datos, informática y sus métodos relacionados\" para \"comprender y analizar fenómenos reales\" con datos.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time"
      ],
      "metadata": {
        "id": "zVdXwsZhDzdN"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conversar(pregunta):\n",
        "  t0 = time()\n",
        "  respuesta = respuesta_chatbot(pregunta)\n",
        "  tf = round(time()-t0,1)\n",
        "  print(f'Tiempo de respuesta: {tf} seg')\n",
        "  print('-----------------------------------------------------------------')\n",
        "  print('Conversación: ',pregunta)\n",
        "  print('-----------------------------------------------------------------')\n",
        "  print(respuesta)\n",
        "  print('-----------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "wVElEooIRJit"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversar('como estas?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvvJ3hO6tuEC",
        "outputId": "0493cc59-da71-41aa-d73e-debb39ecc954"
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta encontrada por el método de comparación de textos - Probabilidad:  1.0\n",
            "Tiempo de respuesta: 6.0 seg\n",
            "-----------------------------------------------------------------\n",
            "Conversación:  como estas?\n",
            "-----------------------------------------------------------------\n",
            "Estoy bien, gracias por preguntar.\n",
            "-----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversar('cursos fundamento')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcjVNIQJEY_F",
        "outputId": "74c539a7-1988-4f7e-c9db-c35e291c5562"
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta encontrada por el método TfidfVectorizer - Probabilidad: 0.82\n",
            "Tiempo de respuesta: 4.7 seg\n",
            "-----------------------------------------------------------------\n",
            "Conversación:  cursos fundamento\n",
            "-----------------------------------------------------------------\n",
            "La ciencia de datos integra el conocimiento del dominio de la aplicación subyacente (por ejemplo, ciencia económica, finanzas, medicina, ciencias naturales, tecnologías de la información) y es un \"concepto consistente en unificar estadística, análisis de datos, informática y sus métodos relacionados\" para \"comprender y analizar fenómenos reales\" con datos.\n",
            "-----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversar('cuentame de la ciencia de datos profe alejo')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBY10R1AqNKK",
        "outputId": "5f8a4e40-c339-4936-8284-bddba6c6e725"
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta encontrada por el método TfidfVectorizer - Probabilidad: 0.93\n",
            "Tiempo de respuesta: 5.4 seg\n",
            "-----------------------------------------------------------------\n",
            "Conversación:  cuentame de la ciencia de datos profe alejo\n",
            "-----------------------------------------------------------------\n",
            "El proceso que sigue un científico de datos para trabajar o resolver problemas con los datos se puede resumir en estos pasos: \n",
            "\n",
            "- Extraer datos, independientemente de la fuente y de su volumen\n",
            "- Limpiar los datos, para eliminar lo que pueda sesgar los resultados\n",
            "- Procesar los datos usando métodos estadísticos como inferencia estadística, modelos de regresión, pruebas de hipótesis, etc\n",
            "- Diseñar experimentos adicionales en caso de ser necesario\n",
            "- Crear visualizaciones gráficas de los datos relevantes de la investigación.\n",
            "-----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversar(\"de que se trata la ciencia de datos y el profe alejo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_wq8HMP4hmU",
        "outputId": "34506efe-eb68-4ddf-f0cb-35a756393b23"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta encontrada por el método TfidfVectorizer - Probabilidad: 0.94\n",
            "Tiempo de respuesta: 6.8 seg\n",
            "-----------------------------------------------------------------\n",
            "Conversación:  de que se trata la ciencia de datos y el profe alejo\n",
            "-----------------------------------------------------------------\n",
            "El proceso que sigue un científico de datos para trabajar o resolver problemas con los datos se puede resumir en estos pasos: \n",
            "\n",
            "- Extraer datos, independientemente de la fuente y de su volumen\n",
            "- Limpiar los datos, para eliminar lo que pueda sesgar los resultados\n",
            "- Procesar los datos usando métodos estadísticos como inferencia estadística, modelos de regresión, pruebas de hipótesis, etc\n",
            "- Diseñar experimentos adicionales en caso de ser necesario\n",
            "- Crear visualizaciones gráficas de los datos relevantes de la investigación.\n",
            "-----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversar('como te llamas?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMMayzPvEadZ",
        "outputId": "1fcfe382-f4e8-4bc1-b7b4-00086c883e27"
      },
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta encontrada por el método de comparación de textos - Probabilidad:  1.0\n",
            "Tiempo de respuesta: 3.9 seg\n",
            "-----------------------------------------------------------------\n",
            "Conversación:  como te llamas?\n",
            "-----------------------------------------------------------------\n",
            "Me llamo X-Bot hijo de Madre. ¿Y tú?\n",
            "-----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversar('como te llamas amigo?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9vUM_wNEkM-",
        "outputId": "d6b2bbca-b563-42ea-b4d8-984d72442434"
      },
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta encontrada por el método de comparación de textos - Probabilidad:  0.94\n",
            "Tiempo de respuesta: 3.9 seg\n",
            "-----------------------------------------------------------------\n",
            "Conversación:  como te llamas amigo?\n",
            "-----------------------------------------------------------------\n",
            "Me llamo X-Bot hijo de Madre, ¡el chatbot más guay del barrio! ¿Y tú?\n",
            "-----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversar('que es un cientifico de datos?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f3e620-a1af-4077-cfef-af16cb52cc06",
        "id": "fcV75wIeE8Zt"
      },
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta encontrada por el método TfidfVectorizer - Probabilidad: 0.84\n",
            "Tiempo de respuesta: 6.9 seg\n",
            "-----------------------------------------------------------------\n",
            "Conversación:  que es un cientifico de datos?\n",
            "-----------------------------------------------------------------\n",
            "Un científico de datos es el profesional que mediante la escritura y aplicación de código de programación y conocimientos en estadística trabaja en la recolección de datos, la limpieza de datos, la exploración de datos, la modelación de datos, visualización de datos, la implementación de soluciones de aprendizaje automático y en la interpretación de resultados.\n",
            "-----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversar('Cómo estás amigo?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUsQBW35SLJB",
        "outputId": "67848628-5d62-43fe-93a2-c80221069ca7"
      },
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta encontrada por el método TfidfVectorizer - Probabilidad: 0.9\n",
            "Tiempo de respuesta: 5.0 seg\n",
            "-----------------------------------------------------------------\n",
            "Conversación:  Cómo estás amigo?\n",
            "-----------------------------------------------------------------\n",
            "La ciencia de datos es diferente de la informática, la estadística y la ciencia de la información, el ganador del premio Turing, Jim Gray, imaginó la ciencia de datos como un \"cuarto paradigma\" de la ciencia (empírico, teórico, computacional y ahora basado en datos) y afirmó que \"todo sobre la ciencia está cambiando debido al impacto de la tecnología de la información\" y la avalancha de datos.\n",
            "-----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversar('La ciencia de datos es')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SrIIgSfZ1mV",
        "outputId": "a04a28eb-0dfd-4e9d-ffc5-ad65d45e6534"
      },
      "execution_count": 263,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta encontrada por el modelo Transformers - tipo: Funcion\n",
            "Tiempo de respuesta: 4.6 seg\n",
            "-----------------------------------------------------------------\n",
            "Conversación:  La ciencia de datos es\n",
            "-----------------------------------------------------------------\n",
            "Estoy familiarizado con una amplia gama de herramientas populares como Python, R, SQL, TensorFlow, scikit-learn, y muchas más. Cuéntame qué necesitas y te ayudaré.\n",
            "-----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversar('La ciencia de datos es')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sT8vwZSTahNB",
        "outputId": "4f7ca3bc-551e-4921-a902-4d45c037a414"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta encontrada por el modelo Transformers - tipo: Funcion\n",
            "Tiempo de respuesta: 6.7 seg\n",
            "-----------------------------------------------------------------\n",
            "Conversación:  La ciencia de datos es\n",
            "-----------------------------------------------------------------\n",
            "Estoy familiarizado con una amplia gama de herramientas populares como Python, R, SQL, TensorFlow, scikit-learn, y muchas más. Cuéntame qué necesitas y te ayudaré.\n",
            "-----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversar('cientifico de datos?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYl4OZoMaxFS",
        "outputId": "e97dff7b-1686-4787-f610-48e6b0c889cd"
      },
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta encontrada por el método TfidfVectorizer - Probabilidad: 0.76\n",
            "Tiempo de respuesta: 4.6 seg\n",
            "-----------------------------------------------------------------\n",
            "Conversación:  cientifico de datos?\n",
            "-----------------------------------------------------------------\n",
            "Un científico de datos es el profesional que mediante la escritura y aplicación de código de programación y conocimientos en estadística trabaja en la recolección de datos, la limpieza de datos, la exploración de datos, la modelación de datos, visualización de datos, la implementación de soluciones de aprendizaje automático y en la interpretación de resultados.\n",
            "-----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversar('que cursos hay')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uG-FGj4IZHf",
        "outputId": "ba516563-27b0-419d-a6d1-77a2f9e2ddca"
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respuesta encontrada por el método TfidfVectorizer - Probabilidad: 0.8\n",
            "Tiempo de respuesta: 4.8 seg\n",
            "-----------------------------------------------------------------\n",
            "Conversación:  que cursos hay\n",
            "-----------------------------------------------------------------\n",
            "La ciencia de datos ha resultado para muchos una disciplina de reciente creación, pero en la realidad este concepto lo utilizó por primera vez el científico danés Peter Naur en la década de los sesenta como sustituto de las ciencias computacionales.\n",
            "-----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ep2fLU0RkUxl"
      },
      "execution_count": 266,
      "outputs": []
    }
  ]
}